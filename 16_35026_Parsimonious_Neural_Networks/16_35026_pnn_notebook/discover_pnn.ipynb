{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering a parsimonious neural network - particle under an external non-linear potential #\n",
    "\n",
    "<i>Saaketh Desai</i>, and <i>Alejandro Strachan</i>, School of Materials Engineering, Purdue University <br>\n",
    "\n",
    "This notebook describes the procedure to train a parsimonious neural network, i.e., a network designed to reproduce the training and testing datasets in the simplest, most interpretable manner possible. We use Keras to train neural networks and the DEAP package for genetic algorithms. The outline of this notebook is:\n",
    "\n",
    "1. Read datasets and split into training and testing sets\n",
    "2. Create a generic model\n",
    "3. Define the objective function for the genetic algorithm\n",
    "4. Set up the genetic algorithm and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.merge import add, concatenate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define some globally relevant parameters and units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = pm; t = fs => v = pm/fs; a = pm/fs^2\n",
    "natoms = 2\n",
    "mass = 26.982\n",
    "timestep = 1.0\n",
    "constant = 9.648532952214415e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read datasets and split into training and testing sets\n",
    "The training and testing data is generated using the LAMMPS software and is stored in the LAMMPS dump file format. The helper function below reads in the file paths to the LAMMPS dump files and then parses them into input and output arrays, where the inputs are position and velocity at time $t$ and the outputs are the position and velocity at time $t + \\Delta t$. We then use the scikit-learn <i> train_test_split()</i> function to split the datasets into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(documents):\n",
    "    input_storage = []; output_storage = []\n",
    "\n",
    "    for file in documents:\n",
    "        with open(file) as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        n=9+natoms\n",
    "        data = data[n-2::n]\n",
    "        data = [x.strip().split() for x in data] # First atom\n",
    "        data = np.array([[float(item) for item in sublist] for sublist in data]) #Turn strings into floats\n",
    "\n",
    "        x = data[:,1].reshape(len(data),1)*100\n",
    "        vx = data[:,4].reshape(len(data),1)/10\n",
    "        fx = (data[:,7]).reshape(len(data),1)\n",
    "\n",
    "        data_xv = np.concatenate((x,vx,fx), axis=1)\n",
    "\n",
    "        input_storage.append(data_xv[:-10:10,:])\n",
    "        output_storage.append(data_xv[10::10,:])\n",
    "    \n",
    "    total_length_list = [len(i) for i in input_storage]\n",
    "    total_length = sum(total_length_list)\n",
    "\n",
    "    inputs = np.zeros((total_length, 3))\n",
    "    outputs = np.zeros((total_length, 3))\n",
    "    start = 0; end = 0\n",
    "    for i in range(len(documents)):\n",
    "        end += len(input_storage[i])\n",
    "        inputs[start:end, :] = input_storage[i]\n",
    "        outputs[start:end, :] = output_storage[i]\n",
    "        start = end\n",
    "\n",
    "    print(\"Inputs\", inputs.shape)\n",
    "    print(\"Outputs\", outputs.shape)\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"../data/Al_3K_small.dump\",\n",
    "             \"../data/Al_273K_small.dump\",\n",
    "             \"../data/Al_530K_small.dump\",\n",
    "             \"../data/Al_703K_small.dump\"]\n",
    "inputs, outputs = read_files(documents)\n",
    "\n",
    "documents = [\"../data/Al_300K_small.dump\"]\n",
    "test_inputs, test_outputs = read_files(documents)\n",
    "\n",
    "train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(inputs[:,0:2], outputs[:,0:2],\n",
    "                                                                        test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a generic model\n",
    "\n",
    "We will first define variables relevant to setting up the model. We then read in the parameters for the pre-trained force model, keeping those weights fixed. The force model is trained outside of this notebook, and is a model that is trained to reproduce the force acting on the atom, given its raw position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = {0: 'linear', 1: 'relu', 2: 'tanh', 3: 'elu'}\n",
    "np.random.seed(100000)\n",
    "weight_dict = {0: 0, 1: 0.5, 2: 1, 3: 2, 4: timestep/2, 5: timestep, 6: 2*timestep, 7: np.random.uniform(-1,1,1)[0]}\n",
    "nact_terms = 6\n",
    "nweight_terms = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../data/force_model_small.h5')\n",
    "w1 = model.layers[1].get_weights()\n",
    "w2 = model.layers[2].get_weights()\n",
    "w3 = model.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few functions help create a generic neural network, the parameters for which will be decided by the genetic algorithm. In the <i> create_model() </i> function, we set the weights and activations according to genes of the individual, as decided by the genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node(input1, input2, name, trainable1, trainable2, act):\n",
    "    base = name\n",
    "    n1 = base + \"1\"\n",
    "    n2 = base + \"2\"\n",
    "    an1 = Dense(1, activation = 'linear', use_bias = False, name=n1, trainable=trainable1) (input1)\n",
    "    an2 = Dense(1, activation = 'linear', use_bias = False, name=n2, trainable=trainable2) (input2)\n",
    "    an = add([an1, an2])\n",
    "    an = Activation(act) (an)\n",
    "    return an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x):\n",
    "    bias_initial = keras.initializers.Zeros()\n",
    "\n",
    "    trainable_list = []\n",
    "    for i in range(nweight_terms):\n",
    "        if (x[i+nact_terms] == 7):\n",
    "            trainable_list.append(True)\n",
    "        else:\n",
    "            trainable_list.append(False)\n",
    "\n",
    "    input_position = Input(shape=(1,))\n",
    "    input_velocity = Input(shape=(1,))\n",
    "\n",
    "    a1 = create_node(input_position, input_velocity, \"a1\", trainable_list[0], trainable_list[1], act_dict[x[0]])\n",
    "    a2 = create_node(input_position, input_velocity, \"a2\", trainable_list[2], trainable_list[3], act_dict[x[1]])\n",
    "    a3 = create_node(a1, a2, \"a3\", trainable_list[6], trainable_list[7], act_dict[x[2]])\n",
    "    a4 = create_node(a1, a2, \"a4\", trainable_list[8], trainable_list[9], act_dict[x[3]])\n",
    "    force_input = create_node(a1, a2, \"fi\", trainable_list[4], trainable_list[5], 'linear')\n",
    "\n",
    "    #force at time t+delta_t/2 from halfstep position\n",
    "    force1 = Dense(10, activation='tanh', use_bias = True, name='force_middle_layer', trainable=False)(force_input)\n",
    "    force2 = Dense(10, activation='tanh', use_bias = True, name='force_middle_layer_2', trainable=False) (force1)\n",
    "    force_output = Dense(1, activation='linear', use_bias = True, name='force_output_layer', trainable=False)(force2)\n",
    "\n",
    "    #a5\n",
    "    a51 = Dense(1, activation = 'linear', use_bias = False, name=\"a51\", trainable=trainable_list[10]) (a3)\n",
    "    a52 = Dense(1, activation = 'linear', use_bias = False, name=\"a52\", trainable=trainable_list[11]) (force_output)\n",
    "    a53 = Dense(1, activation = 'linear', use_bias = False, name=\"a53\", trainable=trainable_list[12]) (a4)\n",
    "    a5 = add([a51, a52, a53])\n",
    "    a5 = Activation(act_dict[x[4]]) (a5)\n",
    "\n",
    "    #a6\n",
    "    a61 = Dense(1, activation = 'linear', use_bias = False, name=\"a61\", trainable=trainable_list[13]) (a3)\n",
    "    a62 = Dense(1, activation = 'linear', use_bias = False, name=\"a62\", trainable=trainable_list[14]) (force_output)\n",
    "    a63 = Dense(1, activation = 'linear', use_bias = False, name=\"a63\", trainable=trainable_list[15]) (a4)\n",
    "    a6 = add([a61, a62, a63])\n",
    "    a6 = Activation(act_dict[x[5]]) (a6)\n",
    "    \n",
    "    #output_position\n",
    "    output_position = create_node(a5, a6, \"op\", trainable_list[16], trainable_list[17], \"linear\")\n",
    "\n",
    "    #output_velocity\n",
    "    output_velocity = create_node(a5, a6, \"ov\", trainable_list[18], trainable_list[19], \"linear\")\n",
    "\n",
    "    model = Model(inputs=[input_position, input_velocity], outputs=[output_position, output_velocity])\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    # FREEZING WEIGHTS\n",
    "    model.layers[16].set_weights(w1)\n",
    "    model.layers[20].set_weights(w2)\n",
    "    model.layers[23].set_weights(w3)\n",
    "    \n",
    "    # SETTING OTHER WEIGHTS\n",
    "    layer_list = []\n",
    "    for i in range(2, 6):\n",
    "        layer_list.append(i)\n",
    "    for i in [10, 11, 14, 15, 17, 18]:\n",
    "        layer_list.append(i)\n",
    "    for i in range(25, 31):\n",
    "        layer_list.append(i)\n",
    "    for i in range(35, 39):\n",
    "        layer_list.append(i)\n",
    "    \n",
    "    for i in range(len(layer_list)):\n",
    "        model.layers[layer_list[i]].set_weights( [ np.array( [[ weight_dict[x[nact_terms+i]] ]] ) ] )\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    return model, trainable_list, layer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the training protocol for the network. Note that training is driven by the genetic algorithm, where the individual's genes decide whether a particular weight is fixed or trainable. If all the weights in the model are fixed, then the model is not trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + ' Loss: ' + str(logs.get('loss')) + '\\n') # Updates current Epoch Number\n",
    "        losses.append(logs.get('loss'))\n",
    "\n",
    "def train(model, train_inputs, train_outputs, val_inputs, val_outputs, verbose=False):\n",
    "    mae_es= keras.callbacks.EarlyStopping(monitor='val_loss', patience=100,\n",
    "                                          min_delta=1e-25, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "    EPOCHS = 20000 # Number of EPOCHS\n",
    "    history = model.fit([train_inputs[:,0], train_inputs[:,1]], [train_outputs[:,0], train_outputs[:,1]], epochs=EPOCHS,\n",
    "                        shuffle=False, batch_size=len(train_inputs), verbose = False, callbacks=[mae_es],\n",
    "                        validation_data = ([val_inputs[:,0], val_inputs[:,1]], [val_outputs[:,0], val_outputs[:,1]]))\n",
    "\n",
    "    if verbose:\n",
    "        plt.figure()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Sq Error')\n",
    "        plt.plot(history.epoch, np.array(history.history['loss']),label='Training loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(w):\n",
    "    if (w == 0):\n",
    "        return 0\n",
    "    elif (w >= 1 and w <= 6):\n",
    "        return 1\n",
    "    elif (w == 7):\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the objective function for the genetic algorithm\n",
    "The objective function consists of three parts: \n",
    "1. The mean squared error of the model on the test set \n",
    "2. A penalty term for non-linear activation functions\n",
    "3. A penalty term for weights that are not fixed, simple values such as 0, 1 or a multiple of an expected weight, such as the timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(individual):\n",
    "    new_model, trainable, layers = create_model(individual)\n",
    "    #print (\"Trainable: \", trainable)\n",
    "    if (any(trainable) == True):\n",
    "        train(new_model, train_inputs, train_outputs, val_inputs, val_outputs, verbose=False)\n",
    "    print(\"Model weights\")\n",
    "    for i in layers:\n",
    "        print (new_model.layers[i].get_weights()[0])\n",
    "\n",
    "    loss, mse_test_x, mse_test_v = new_model.evaluate([test_inputs[:,0:1], test_inputs[:,1:2]],\n",
    "                                                      [test_outputs[:,0:1], test_outputs[:,1:2]], verbose=0)\n",
    "    actfunc_term = [i**2 for i in individual[:nact_terms]]\n",
    "    weights = individual[nact_terms:]\n",
    "    weight_term = 0\n",
    "    for j in range(nweight_terms):\n",
    "        weight_term += f3(weights[j])\n",
    "\n",
    "    mse_test_term_x = 10*np.log10(mse_test_x)\n",
    "    mse_test_term_v = 10*np.log10(mse_test_v)\n",
    "\n",
    "    obj = mse_test_term_x + mse_test_term_v + np.sum(actfunc_term) + weight_term\n",
    "    print (\"Individual: \", individual, flush=True)\n",
    "    print (\"Objective function: \", mse_test_x, mse_test_v, np.sum(actfunc_term), weight_term, obj, flush=True)\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    return (obj,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up the genetic algorithm and saving the results\n",
    "\n",
    "Each network is expressed as an individual of 26 genes, the genes representing the possible activations and weights. We thus define an individual to be a custom container, which is repeated to create a population. For details on this, please refer to the DEAP guide on setting up a genetic algorithm, which can be found [here](https://deap.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### DEAP #####################\n",
    "#create fitness class and individual class\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "#toolbox.register(\"map\", futures.map)\n",
    "\n",
    "def custom_initRepeat(container, func, max1, max2, n):\n",
    "    func_list = []\n",
    "    for i in range(n):\n",
    "        if (i < nact_terms):\n",
    "            func_list.append(func(0, max1))\n",
    "        else:\n",
    "            func_list.append(func(0, max2))\n",
    "    return container(func_list[i] for i in range(n))\n",
    "\n",
    "toolbox.register(\"create_individual\", custom_initRepeat, creator.Individual, random.randint,\n",
    "                 max1=3, max2=7, n=nact_terms+nweight_terms)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.create_individual)\n",
    "\n",
    "def custom_mutation(individual, max1, max2, indpb):\n",
    "    size = len(individual)\n",
    "    for i in range(size):\n",
    "        if random.random() < indpb:\n",
    "            if (i < nact_terms):\n",
    "                individual[i] = random.randint(0, max1)\n",
    "            else:\n",
    "                individual[i] = random.randint(0, max2)\n",
    "    return individual,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the two point crossover method for mating two individuals, and perform a random mutation using the custom mutation function. We then define a population size of 200 and define the statistics that we wish to log in the output of the code. The stats object decides which quantities are saved to the logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxpb = 0.5\n",
    "mutpb = 0.3\n",
    "ngens = 50\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", custom_mutation, max1=3, max2=7, indpb=mutpb)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=10)\n",
    "toolbox.register(\"evaluate\", objective_function)\n",
    "\n",
    "\n",
    "random.seed(100000)\n",
    "population = toolbox.population(n=200)\n",
    "\n",
    "hof = tools.HallOfFame(1)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "pop, logbook = algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngens, stats=stats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
