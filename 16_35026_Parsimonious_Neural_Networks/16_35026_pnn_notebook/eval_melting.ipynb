{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a parsimonious neural network (PNN) model - melting #\n",
    "\n",
    "<i>Saaketh Desai</i>, and <i>Alejandro Strachan</i>, School of Materials Engineering, Purdue University <br>\n",
    "\n",
    "This notebook evaluates a parsimonious neural network generated by the optimization algorithm in the [previous notebook](discover_melting.ipynb). The outline of this notebook is:\n",
    "\n",
    "1. Read training and testing data\n",
    "2. Create model and set weights to values obtained from optimization \n",
    "3. Express network weights as interpretable equations\n",
    "4. Evaluate model metrics on datasets and the objective function as per the genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.layers import Dense, Input, Activation, multiply\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.merge import add, concatenate\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read training and testing data##\n",
    "We read in a CSV file containing the fundamental quantities such as bulk modulus, shear modulus, density etc., along with the experimental melting temperature. We then compute quantities such as effective sound speed ($v_m$) to compute effective temperatures $\\theta_0, \\theta_1, \\theta_2, \\theta_3$ and normalized inputs $\\theta_1', \\theta_2', \\theta_3'$. Finally, we use the `train_test split()` method from scikit-learn to split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Combined_data_v3.csv\")\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 6.62607015*1e-34\n",
    "k = 1.380649*1e-23\n",
    "Na = 6.0221407*1e23\n",
    "pi = np.pi\n",
    "hbar = 1.054571817*1e-34\n",
    "\n",
    "vs = np.sqrt(df['G_VRH']/df['density']) #from Zack\n",
    "vp = np.sqrt((df['K_VRH'] + (4/3)*df['G_VRH'])/df['density']) #from Zack\n",
    "vm = ( 3/( (1/vp)**3 + 2*(1/vs)**3 ) )**(1/3) #from JP Poirier paper\n",
    "\n",
    "df['debye_temp'] = 10**13*(h/k)*(3/(4*pi*df['volume_per_atom']))**(1/3)*vm\n",
    "\n",
    "df['a'] = (df['volume_per_atom'])**(1/3)\n",
    "\n",
    "a = df['a']\n",
    "m = df['mean_mass']\n",
    "G = df['G_VRH']\n",
    "K = df['K_VRH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = (1.054571817/1.380649)*100*vm/a #hcross*vm/(k*a)\n",
    "theta1 = (1.054571817**2*6.0221407/1.380649)*10*(1/(m*a**2)) #hcross**2/(m*a**2*k)\n",
    "theta2 = (1/1.380649)*100*(a**3*G) #a**3*G/k\n",
    "theta3 = (1/1.380649)*100*(a**3*K) #a**3*K/k\n",
    "\n",
    "theta1_prime = theta1/theta0\n",
    "theta2_prime = theta2/theta0\n",
    "theta3_prime = theta3/theta0\n",
    "\n",
    "ones = np.ones(len(theta1_prime))\n",
    "\n",
    "Tm_prime = df['Tm']/theta0\n",
    "\n",
    "inputs = np.array([theta1_prime, theta2_prime, theta3_prime, ones], dtype='float')\n",
    "inputs = inputs.T\n",
    "outputs = np.array(Tm_prime).reshape(-1, 1)\n",
    "\n",
    "print (inputs.shape, outputs.shape)\n",
    "\n",
    "train_inputs, test_inputs, train_outputs, test_outputs = train_test_split(inputs, outputs, test_size=0.2, random_state=0)\n",
    "print (train_inputs.shape, train_outputs.shape)\n",
    "print (test_inputs.shape, test_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create model and set weights to values obtained from optimization##\n",
    "We now create the generic neural network (used as a starting point during the equation discovery process) and set the activations and weights to be the values obtained from the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = {0: 'linear', 1: 'squared', 2: 'inverse', 3: 'multiply', 4: 'tanh'}\n",
    "np.random.seed(100000)\n",
    "weight_dict = {0: 0, 1: 1, 2: np.random.uniform(-1,1,1)[0]}\n",
    "nact_terms = 4\n",
    "nweight_terms = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect act, weights and biases from individual\n",
    "individual = [0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 2, 0, 2]\n",
    "weights_list =  \"0.         0.         0.03156352 1.         0.         0. \\\n",
    "0.         0.         0.         0.03156353 0.0120849  0. \\\n",
    "0.91340095\"\n",
    "\n",
    "weights_list = [float(x) for x in weights_list.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_act(x):\n",
    "    return x*x\n",
    "\n",
    "def inverse_act(x):\n",
    "    return 1/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node(input1, input2, input3, name, trainable1, trainable2, trainable3, act, x, idx):\n",
    "    base = name\n",
    "    n1 = base + \"1\"\n",
    "    n2 = base + \"2\"\n",
    "    n3 = base + \"3\"\n",
    "    an1 = Dense(1, activation = 'linear', use_bias = False, name=n1, trainable=trainable1) (input1)\n",
    "    an2 = Dense(1, activation = 'linear', use_bias = False, name=n2, trainable=trainable2) (input2)\n",
    "    an3 = Dense(1, activation = 'linear', use_bias = False, name=n3, trainable=trainable3) (input3)\n",
    "    \n",
    "    node_list = [an1, an2, an3]\n",
    "    if (act == \"multiply\"):\n",
    "        non_zero_list = []\n",
    "        zero_list = []\n",
    "        for i, j in enumerate(node_list):\n",
    "            if (x[idx+i] == 1 or x[idx+i] == 2):\n",
    "                non_zero_list.append(j)\n",
    "            else:\n",
    "                zero_list.append(j)\n",
    "        if ( len(non_zero_list) == 0 ):\n",
    "            non_zero_list = node_list\n",
    "            an = multiply(non_zero_list)\n",
    "        if ( len(non_zero_list) == 1 ):\n",
    "            anx = non_zero_list[0]\n",
    "            an = add([anx, zero_list[0], zero_list[1]])\n",
    "        else:\n",
    "            an = multiply(non_zero_list)\n",
    "    else:\n",
    "        an = add(node_list)\n",
    "        if (act == \"squared\"):\n",
    "            an = Activation(squared_act) (an)\n",
    "        elif (act == \"inverse\"):\n",
    "            an = Activation(inverse_act) (an)\n",
    "        else:\n",
    "            an = Activation(act) (an)\n",
    "    return an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x):\n",
    "    #initializer = keras.initializers.RandomUniform(minval=-0.001, maxval=0.001, seed=0)\n",
    "    bias_initial = keras.initializers.Zeros()\n",
    "\n",
    "    trainable_list = []\n",
    "    for i in range(nweight_terms):\n",
    "        if (x[i+nact_terms] == 2):\n",
    "            trainable_list.append(True)\n",
    "        else:\n",
    "            trainable_list.append(False)\n",
    "\n",
    "    input1 = Input(shape=(1,))\n",
    "    input2 = Input(shape=(1,))\n",
    "    input3 = Input(shape=(1,))\n",
    "    input4 = Input(shape=(1,))\n",
    "\n",
    "    a1 = create_node(input1, input2, input3, \"a1\", trainable_list[0], trainable_list[1], \n",
    "                     trainable_list[2], act_dict[x[0]], x, 0+nact_terms)\n",
    "    a2 = create_node(input1, input2, input3, \"a2\", trainable_list[3], trainable_list[4], \n",
    "                     trainable_list[5], act_dict[x[1]], x, 3+nact_terms)\n",
    "    a3 = create_node(input1, input2, input3, \"a3\", trainable_list[6], trainable_list[7], \n",
    "                     trainable_list[8], act_dict[x[2]], x, 6+nact_terms)\n",
    "\n",
    "    an1 = Dense(1, activation = 'linear', use_bias = False, name='output1', trainable=trainable_list[9]) (a1)\n",
    "    an2 = Dense(1, activation = 'linear', use_bias = False, name='output2', trainable=trainable_list[10]) (a2)\n",
    "    an3 = Dense(1, activation = 'linear', use_bias = False, name='output3', trainable=trainable_list[11]) (a3)\n",
    "\n",
    "    an4 = Dense(1, activation = 'linear', use_bias = False, name='output4', trainable=trainable_list[12]) (input4)\n",
    "\n",
    "    act = act_dict[x[3]]\n",
    "    node_list = [an1, an2, an3, an4]\n",
    "    if (act == \"multiply\"):\n",
    "        non_zero_list = []\n",
    "        zero_list = []\n",
    "        for i, j in enumerate(node_list):\n",
    "            if (x[9+i] == 1 or x[9+i] == 2):\n",
    "                non_zero_list.append(j)\n",
    "            else:\n",
    "                zero_list.append(j)\n",
    "        if ( len(non_zero_list) == 0 ):\n",
    "            non_zero_list = node_list\n",
    "            an = multiply(non_zero_list)\n",
    "        elif ( len(non_zero_list) == 1 ):\n",
    "            anx = non_zero_list[0]\n",
    "            an = add([anx, zero_list[0], zero_list[1], zero_list[2]])\n",
    "        else:\n",
    "            an = multiply(non_zero_list)\n",
    "    else:\n",
    "        an = add(node_list)\n",
    "        if (act == \"squared\"):\n",
    "            an = Activation(squared_act) (an)\n",
    "        elif (act == \"inverse\"):\n",
    "            an = Activation(inverse_act) (an)\n",
    "        else:\n",
    "            an = Activation(act) (an)\n",
    "    output = an\n",
    "\n",
    "    model = Model(inputs=[input1, input2, input3, input4], outputs=[output])\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    layer_list = []\n",
    "    for i in range(len(model.layers)):\n",
    "        name = model.layers[i].name\n",
    "        if ( (\"activation\" in name) or (\"input\" in name) or (\"add\" in name) or (\"multiply\" in name) ):\n",
    "            continue\n",
    "        else:\n",
    "            layer_list.append(i)\n",
    "    \n",
    "    for i in range(len(layer_list)):\n",
    "        #model.layers[layer_list[i]].set_weights( [ np.array( [[ weight_dict[x[nact_terms+i]] ]] ) ] )\n",
    "        model.layers[layer_list[i]].set_weights( [ np.array( [[ weights_list[i] ]] ) ] )\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    return model, trainable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(w):\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model, trainable = create_model(individual)\n",
    "#new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = new_model.get_weights()\n",
    "weight_list = []\n",
    "for weight in weights:\n",
    "    weight_list.append(weight[0][0])\n",
    "weight_list = np.array(weight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Express network weights as interpretable equations##\n",
    "We use the sympy library to decode the weights and activations of the network into equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of layer names in model\n",
    "name_list = []\n",
    "for i in range(len(new_model.layers)):\n",
    "    name = new_model.layers[i].name\n",
    "    if ( (\"activation\" in name) or (\"input\" in name) or (\"add\" in name) or (\"multiply\" in name) ):\n",
    "        continue\n",
    "    else:\n",
    "        name_list.append(name)\n",
    "print (name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set activation for each neuron\n",
    "act_nodes = {\"a1\": act_dict[individual[0]], \"a2\": act_dict[individual[1]], \"a3\": act_dict[individual[2]], \n",
    "             \"output\": act_dict[individual[3]]}\n",
    "\n",
    "weight_dict = {}\n",
    "for i, weight in enumerate(weight_list):\n",
    "    name = name_list[i]\n",
    "    weight_dict[name] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define symbolic variables\n",
    "\n",
    "from sympy import *\n",
    "\n",
    "theta1_prime, theta2_prime, theta3_prime = symbols('theta1_prime theta2_prime theta3_prime')\n",
    "#Symbolically evaluate each node\n",
    "def return_value(i1, i2, i3, act, name):\n",
    "    n1 = name + \"1\"\n",
    "    n2 = name + \"2\"\n",
    "    n3 = name + \"3\"\n",
    "    if (act == 'linear'):\n",
    "        value = i1*weight_dict[n1] + i2*weight_dict[n2] + i3*weight_dict[n3]\n",
    "    elif (act == 'squared'):\n",
    "        value = ( i1*weight_dict[n1] + i2*weight_dict[n2] + i3*weight_dict[n3] )**2\n",
    "    elif (act == 'multiply'):\n",
    "        value = i1*weight_dict[n1] * i2*weight_dict[n2] * i3*weight_dict[n3]\n",
    "    elif (act == 'inverse'):\n",
    "        value = 1/( i1*weight_dict[n1] + i2*weight_dict[n2] + i3*weight_dict[n3] )\n",
    "    elif (act == 'tanh'):\n",
    "        value = tanh( i1*weight_dict[n1] + i2*weight_dict[n2] + i3*weight_dict[n3] ) \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect expressions for node a1, a2, a3\n",
    "\n",
    "a1 = return_value(theta1_prime, theta2_prime, theta3_prime, act_nodes[\"a1\"], \"a1\")\n",
    "a2 = return_value(theta1_prime, theta2_prime, theta3_prime, act_nodes[\"a2\"], \"a2\")\n",
    "a3 = return_value(theta1_prime, theta2_prime, theta3_prime, act_nodes[\"a3\"], \"a3\")\n",
    "\n",
    "#Symbolically evaluate output\n",
    "name = \"output\"\n",
    "\n",
    "n1 = name + \"1\"; n2 = name + \"2\"; n3 = name + \"3\"; n4 = name + \"4\"\n",
    "act = act_nodes[\"output\"]\n",
    "if (act == 'linear'):\n",
    "    value = a1*weight_dict[n1] + a2*weight_dict[n2] + a3*weight_dict[n3] + 1*weight_dict[n4]\n",
    "elif (act == 'squared'):\n",
    "    value = ( a1*weight_dict[n1] + a2*weight_dict[n2] + a3*weight_dict[n3] + 1*weight_dict[n4] )**2\n",
    "elif (act == 'multiply'):\n",
    "    value = a1*weight_dict[n1] * a2*weight_dict[n2] * a3*weight_dict[n3] * 1*weight_dict[n4]\n",
    "elif (act == 'inverse'):\n",
    "    value = 1/( a1*weight_dict[n1] + a2*weight_dict[n2] + a3*weight_dict[n3] + 1*weight_dict[n4] )\n",
    "elif (act == 'tanh'):\n",
    "    value = tanh( a1*weight_dict[n1] + a2*weight_dict[n2] + a3*weight_dict[n3] + 1*weight_dict[n4] ) \n",
    "\n",
    "output = value\n",
    "\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate model on dataset and evaluate objective function ##\n",
    "We now evaluate the model discovered on the testing dataset using the metrics defined during the training process. Lastly, we evaluate the objective function of our model, as seen by the genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e67c1b7dd87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalid_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Evaluate model on train/test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m mse_train = new_model.evaluate([train_inputs[:, 0], train_inputs[:, 1], train_inputs[:, 2], train_inputs[:, 3]], \n\u001b[0m\u001b[1;32m      4\u001b[0m                                train_outputs, verbose=0)\n\u001b[1;32m      5\u001b[0m mse_test = new_model.evaluate([test_inputs[:, 0], test_inputs[:, 1], test_inputs[:, 2], test_inputs[:, 3]], \n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "valid_flag = True\n",
    "#Evaluate model on train/test sets\n",
    "mse_train = new_model.evaluate([train_inputs[:, 0], train_inputs[:, 1], train_inputs[:, 2], train_inputs[:, 3]], \n",
    "                               train_outputs, verbose=0)\n",
    "mse_test = new_model.evaluate([test_inputs[:, 0], test_inputs[:, 1], test_inputs[:, 2], test_inputs[:, 3]], \n",
    "                              test_outputs, verbose=0)\n",
    "\n",
    "if (np.isnan(mse_train) or np.isnan(mse_test) or np.isinf(mse_train) or np.isinf(mse_test)):\n",
    "    valid_flag = False\n",
    "\n",
    "weights = new_model.get_weights()\n",
    "weight_list = []\n",
    "for weight in weights:\n",
    "    weight_list.append(weight[0][0])\n",
    "weight_list = np.array(weight_list)\n",
    "\n",
    "#handle nan weights\n",
    "if (np.isnan(weight_list).any()):\n",
    "    valid_flag = False\n",
    "\n",
    "if (valid_flag):\n",
    "    pass\n",
    "    #print (weight_list)\n",
    "else:\n",
    "    mse_test = 1e50\n",
    "\n",
    "actfunc_term = [i**2 for i in individual[:nact_terms]]\n",
    "weights = individual[nact_terms:]\n",
    "weight_term = 0\n",
    "for j in range(nweight_terms):\n",
    "    weight_term += f3(weights[j])\n",
    "\n",
    "mse_test_term = np.log10(mse_test)\n",
    "\n",
    "obj = mse_test_term + 0.002*(np.sum(actfunc_term) + weight_term)\n",
    "#print (\"Individual: \", individual, flush=True)\n",
    "print (\"Objective function: \", mse_test, np.sum(actfunc_term), weight_term, obj, flush=True)\n",
    "\n",
    "# Parsimonious neural networks learn non-linear interpretable laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.sqrt(mse_train), np.sqrt(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our predictions vs the ground truth data in a parity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = new_model.predict([test_inputs[:, 0], test_inputs[:, 1], test_inputs[:, 2], test_inputs[:, 3]])\n",
    "train_preds = new_model.predict([train_inputs[:, 0], train_inputs[:, 1], train_inputs[:, 2], train_inputs[:, 3]])\n",
    "\n",
    "plt.plot(train_preds, train_outputs, 'o')\n",
    "plt.plot(test_preds, test_outputs, 'o')\n",
    "x = np.linspace(min(train_preds), max(train_preds), 500)\n",
    "plt.plot(x, x)\n",
    "\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
