{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms for Ionic Conductivity of LLZO-type Garnets\n",
    "\n",
    "### Authorship and credits\n",
    "\n",
    "<b> nanoHUB tools by: </b>  <i>Juan Carlos Verduzco</i> and <i>Alejandro Strachan</i>, Materials Engineering, Purdue University <br>\n",
    "<b> Database curated by: </b> <i>Juan Carlos Verduzco</i>, Materials Engineering, Purdue University <br>\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we include extra Figures for publication as \"An active learning approach for the design of doped LLZO ceramic garnets for battery applications\" (Submitted)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Querying / Processing Data <br>\n",
    "2. Obtaining features/descriptors from Matminer <br>\n",
    "3. Regression Models <br>\n",
    "    3.1 Neural Network <br>\n",
    "    3.2 Random Forests with Sample Uncertainties - Residual Analysis <br>\n",
    "    3.3 Random Forests with Sample Uncertainties - Model Predictions  <br>\n",
    "4. Ta Analysis <br>\n",
    "5. Active Learning Approach <br>\n",
    "6. Active Learning - 30 Oldest Points <br>\n",
    "7. Active Learning - 30 Random Points <br>\n",
    "8. Garnet Predictor for codoped LLZO <br>\n",
    "\n",
    "Notes: This notebook uses tools from [Citrination](https://citrination.com/) and requires an account with an API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "This notebook requires several libraries to be installed. They are separated in blocks depending on their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING (MATPLOTLIB)\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from lolopy.learners import RandomForestRegressor\n",
    "\n",
    "# PYTHON\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# MACHINE LEARNING\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "print(keras.__version__)\n",
    "\n",
    "# CITRINATION / MATMINER\n",
    "\n",
    "from matminer.data_retrieval.retrieve_Citrine import CitrineDataRetrieval\n",
    "from matminer.featurizers.base import MultipleFeaturizer\n",
    "from matminer.featurizers import composition as cf\n",
    "from sklearn.model_selection import KFold\n",
    "from pymatgen import Composition\n",
    "from scipy.stats import norm\n",
    "\n",
    "# PLOTTING (PLOTLY)\n",
    "import plotly \n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "# This snipped refers to the adding of the CitrineKey on the main page of the tool. If you are running this notebook by itself, please comment it out and write your citrinekey in the cell below.\n",
    "file = open(os.path.expanduser('~/.citrinetools.txt'),\"r+\")\n",
    "apikey = file.readline()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Querying a Database from Citrination\n",
    "\n",
    "Matminer offers API tools to facilitate querying of databases like the Materials Project and Citrination. An individual **Citrine Key** is required for the query command <i>CitrineDataRetrieval</i>.\n",
    "\n",
    "Data is stored in a Pandas Dataframe and the list of possible properties to be queried can be consulted by setting the print_properties_options parameter to **True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cdr = CitrineDataRetrieval(apikey) # Citrine Key\n",
    "\n",
    "data = cdr.get_dataframe(criteria={'data_set_id': 184812}, print_properties_options=False) # LLZO Database\n",
    "property_interest = 'Ionic Conductivity' # Property to be queried\n",
    "\n",
    "display(data.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a utility function that will transform the <i>chemicalFormula</i> column into a Matminer composition object, which will be then used to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_composition(c): # Function to get compositions from chemical formula using pymatgen\n",
    "    try:\n",
    "        return Composition(c)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the utility function to transform the <i>chemicalFormula</i> column, and we'll typecast relevant columns into numeric types.\n",
    "<br>\n",
    "For this specific application, we'll introduce some filters for the dataframe. We are interested in measurements in structures that are cubic and measured at room temperature (Defined as 18°C < T < 30°C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['composition'] = data['chemicalFormula'].apply(get_composition) # Transformation of chemicalformula string into Matminer composition\n",
    "data['Measuring Temperature'] = pd.to_numeric(data['Measuring Temperature'], errors='coerce') # Transformation of Measuring Temp dataframe column from type <str> to a numberic type <int>\n",
    "data[property_interest] = pd.to_numeric(data[property_interest], errors='coerce') # Transformation of our property of interest dataframe column from type <str> to a numberic type <int>\n",
    "data[\"Year Published\"] = pd.to_numeric(data[\"Year Published\"], errors='coerce') # Transformation of our property of interest dataframe column from type <str> to a numberic type <int>\n",
    "\n",
    "data = data[data['Crystallographic Structure'] == 'Cubic'] # Filter all non-cubic structures\n",
    "data = data[data['Measuring Temperature']<30] # Filter all high temperature measurements (over room temperature)\n",
    "data = data[data['Measuring Temperature']>18] # Filter all low temperature measurements (over room temperature)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True) # Reindexing of dataframe rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before removing duplicates, we will store all the experimental values for compositions that include Tantalum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_indexes = []\n",
    "for _ in range(len(data)):\n",
    "    if (\"Zr\" in data['composition'][_] and \"Ta\" in data['composition'][_] and len(data['composition'][_])==5):\n",
    "        ta_indexes.append(_)\n",
    "    elif (\"Zr\" in data['composition'][_] and len(data['composition'][_])==4):\n",
    "        ta_indexes.append(_) \n",
    "        \n",
    "ta_dataframe = data[data.index.isin(ta_indexes)]\n",
    "#display(ta_dataframe)        \n",
    "        \n",
    "x_values_exp = [_[\"Ta\"] for _ in list(ta_dataframe['composition'])]\n",
    "y_values_exp = list(ta_dataframe['Ionic Conductivity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce noise in the neural network and deal with the inconsistencies in the data, we will filter repeated composition values from different measurements and replace the value for ionic conductivity with the median of the values. Similar approaches have been implemented in this [paper](https://iopscience.iop.org/article/10.1088/1361-651X/aaf8ca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_indexes = data[data.duplicated(subset = data.columns.tolist()[0], keep=False)].index.tolist()\n",
    "\n",
    "dup_dataframe =data[data.duplicated(subset = data.columns.tolist()[0], keep=False)]\n",
    "\n",
    "duplicates = [[dup_dataframe.iloc[x][0], dup_dataframe.iloc[x][4], dup_indexes[x], dup_dataframe.iloc[x][-2]] for x in range(len(dup_dataframe.index))]\n",
    "duplicate_compositions = {k: [] for k in set([dup_dataframe.iloc[x][0] for x in range(len(dup_dataframe.index))])}\n",
    "duplicate_indexes = {k: [] for k in set([dup_dataframe.iloc[x][0] for x in range(len(dup_dataframe.index))])}\n",
    "duplicate_years = {k: [] for k in set([dup_dataframe.iloc[x][0] for x in range(len(dup_dataframe.index))])}\n",
    "\n",
    "for _ in duplicates:\n",
    "    duplicate_compositions[_[0]].append(_[1])\n",
    "    duplicate_indexes[_[0]].append(_[2]) \n",
    "    duplicate_years[_[0]].append(_[3])     \n",
    "\n",
    "for k in duplicate_compositions:\n",
    "    \n",
    "    duplicate_compositions[k] = np.median(duplicate_compositions[k])\n",
    "    data.at[duplicate_indexes[k][0], 'Ionic Conductivity'] = duplicate_compositions[k]  \n",
    "\n",
    "    duplicate_years[k] = np.min(duplicate_years[k])\n",
    "    data.at[duplicate_indexes[k][0], 'Year Published'] = duplicate_years[k]      \n",
    "    data = data.drop(duplicate_indexes[k][1:], axis = 0)\n",
    "\n",
    "data = data.reset_index()\n",
    "data = data.drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates, we will query the dataset for the values that were substituted for the Tantalum compositions, the median of the experimental values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_indexes = []\n",
    "for _ in range(len(data)):\n",
    "    if (\"Zr\" in data['composition'][_] and \"Ta\" in data['composition'][_] and len(data['composition'][_])==5):\n",
    "        ta_indexes.append(_)\n",
    "    elif (\"Zr\" in data['composition'][_] and len(data['composition'][_])==4):\n",
    "        ta_indexes.append(_)        \n",
    "        \n",
    "ta_dataframe = data[data.index.isin(ta_indexes)]\n",
    "#display(ta_dataframe)\n",
    "\n",
    "x_values_dupmed = [_[\"Ta\"] for _ in list(ta_dataframe['composition'])]\n",
    "y_values_dupmed = list(ta_dataframe['Ionic Conductivity'])\n",
    "\n",
    "sort_dupmed = list(zip(x_values_dupmed, y_values_dupmed))\n",
    "sort_dupmed = sorted(sort_dupmed, key = lambda t: t[0])\n",
    "\n",
    "x_values_dupmed = [item[0] for item in sort_dupmed ] \n",
    "y_values_dupmed = [item[1] for item in sort_dupmed ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell produces a breakdown of the number of elements in the oxides compositions and a distribution of the elements present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "freq = data[\"composition\"]\n",
    "list_freq = []\n",
    "\n",
    "for _ in freq:\n",
    "    a = [str(x) for x in _]\n",
    "    list_freq.append(a)\n",
    "    \n",
    "list_freq_flat = [item for sublist in list_freq for item in sublist]  \n",
    "listfreqctr = collections.Counter(list_freq_flat)\n",
    "print(listfreqctr)\n",
    "    \n",
    "lengths = list(map(len,list_freq))\n",
    "lenctr = collections.Counter(lengths)\n",
    "\n",
    "print(lenctr)\n",
    "# print(type(freq[0]))\n",
    "# print(freq[0])\n",
    "# print(list(freq[0])[0])\n",
    "\n",
    "# print(type(list(freq[0])[0]))\n",
    "# print(str(list(freq[0])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Matminer Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  MultipleFeaturizer([cf.Stoichiometry(), cf.ElementProperty.from_preset(\"magpie\"), cf.ValenceOrbital(props=['avg']), cf.ElementFraction()]) # Featurizers\n",
    "\n",
    "X = np.array(f.featurize_many(data['composition'], ignore_errors=True)) # Array to store such features\n",
    "\n",
    "measuring_temp_array = np.array(data['Measuring Temperature']).reshape(-1,1) # Here we are stacking the Measuring temperature numpy array into the features previously calculated to add it as a descriptor. \n",
    "X = np.hstack((X,measuring_temp_array))\n",
    "\n",
    "y = data[property_interest].values # Separate the value we want to predict to use as labels.\n",
    "years = data[\"Year Published\"].values\n",
    "\n",
    "# This code is to drop columns with std = 0. \n",
    "x_df = pd.DataFrame(X)\n",
    "x_df = x_df.loc[:, x_df.std() != 0]\n",
    "print(x_df.shape) # This shape is (#Entries, #Descriptors per entry)\n",
    "\n",
    "# This code is to drop columns with std = 0. \n",
    "x_df_prior = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 3. Regression Models\n",
    "\n",
    "We will start by creating a models for regression with all these entries and descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Neural Networks\n",
    "\n",
    "\n",
    "We set the architecture of the sequential feed-forward neural network we'll test. Weights are initialized with a Random Normal distribution and biases are initialized at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this training function with a validation mean absolute error (mae) stopping function to train the model. A 10% validation set is set to be taken from the training.\n",
    "<br>\n",
    "A figure of training mae vs validation mae is shown. Overfitting occurs when the validation mae starts to increase, so we revert the weights to those of the best epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "# DATA SPLIT AND NORMALIZATION\n",
    "all_values, all_labels = shuffle(all_values, all_labels, random_state=4)\n",
    "\n",
    "train_values, test_values = np.split(all_values, [index_split_at])\n",
    "train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "feature_mean = np.mean(train_values, axis=0)\n",
    "feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "train_values = (train_values - feature_mean)/ (feature_std)\n",
    "test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "neuralnetwork_model = Sequential()\n",
    "neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "neuralnetwork_model.add(Dropout(0.2))\n",
    "neuralnetwork_model.add(Dense(120, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "neuralnetwork_model.add(Dropout(0.2))\n",
    "neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "neuralnetwork_model.add(Dropout(0.2))\n",
    "neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "\n",
    "neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "[loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "    \n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can now make predictions for our entry values. In this match plot we are analyzing the real value of the label vs the prediction of the trained model. Values that lay on the match line at x=y are accurately predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = neuralnetwork_model.predict(test_values).flatten() # Prediction of the test set\n",
    "\n",
    "values = np.concatenate((train_values, test_values), axis=0) # This line joins the values together to evaluate all of them\n",
    "all_predictions = neuralnetwork_model.predict(values).flatten()\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.errorbar(all_labels, all_predictions, color='green', marker='o', markersize=12, linestyle='None', label='Training Data')\n",
    "plt.errorbar(test_labels, test_predictions, color='red', marker='o', markersize=12, linestyle='None',label='Testing Data')\n",
    "plt.plot([-1, 20], [-1, 20], linestyle='dashed', color='black')\n",
    "plt.xticks(np.linspace(0,20,11),fontsize=26)\n",
    "plt.yticks(np.linspace(0,20,11), fontsize=26)\n",
    "plt.xlim([-1,20])\n",
    "plt.ylim([-1,20])\n",
    "plt.grid()\n",
    "plt.legend(loc=2, fontsize=22)\n",
    "plt.ylabel('Predicted Conductivity x10$^{-4}$ (S/cm)', fontsize=26)\n",
    "plt.xlabel('Experimental Conductivity x10$^{-4}$ (S/cm)', fontsize=26) \n",
    "plt.title('Artificial Neural Network', fontsize=26)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forests with Sample Uncertainties (FUELS Framework) -- Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-process the data\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "from lolopy.learners import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = RandomForestRegressor(num_trees=500)\n",
    "\n",
    "# KFOLD CROSS VALIDATION TO CALCULATE RESIDUALS\n",
    "\n",
    "y_resid = []\n",
    "y_uncer = []\n",
    "for train_id, test_id in KFold(10, shuffle=True).split(all_values):\n",
    "    model.fit(all_values[train_id], all_labels[train_id])\n",
    "    yf_pred, yf_std = model.predict(all_values[test_id], return_std=True)\n",
    "    y_resid.extend(yf_pred - all_labels[test_id])\n",
    "    y_uncer.extend(yf_std)\n",
    "    \n",
    "resid = np.divide(y_resid, y_uncer)\n",
    "\n",
    "x_ev = np.linspace(-5, 5, 50)\n",
    "\n",
    "# PLOTTING\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.hist(resid, density=True, label='Residual \\nDensity', bins=10)\n",
    "plt.plot(x_ev, norm.pdf(x_ev), color='black', linewidth=4, linestyle='-', label='Normal \\nDistribution') # NORMAL DISTRIBUTION\n",
    "plt.xticks(fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.xlim([-5,5])\n",
    "plt.grid()\n",
    "plt.legend(fontsize=22)\n",
    "plt.ylabel('Probability Density', fontsize=26)\n",
    "plt.xlabel('RF Normalized Residual', fontsize=26)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forests with Sample Uncertainties (FUELS Framework) -- MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll re-process the data\n",
    "\n",
    "from lolopy.learners import RandomForestRegressor\n",
    "from lolopy.metrics import root_mean_squared_error\n",
    "\n",
    "history_table = pd.DataFrame(columns=['Composition','Experimental',\"Prediction\"])\n",
    "\n",
    "compositions_examined = np.array(data[\"chemicalFormula\"], dtype = \"object\") # Storing the composition together with \n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "\n",
    "# SHUFFLE AND SPLITTING\n",
    "\n",
    "all_values, all_labels, compositions_examined = shuffle(all_values, all_labels, compositions_examined, random_state=6)\n",
    "\n",
    "train_values, test_values = np.split(all_values, [index_split_at])\n",
    "train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "comp_train, comp_test = np.split(compositions_examined, [index_split_at])\n",
    "\n",
    "# MODEL AND TRAINING\n",
    "\n",
    "randomforest_model = RandomForestRegressor(num_trees=500)\n",
    "randomforest_model.fit(train_values, train_labels)\n",
    "\n",
    "test_pred, test_std = randomforest_model.predict(test_values, return_std=True)\n",
    "all_pred, all_std = randomforest_model.predict(all_values, return_std=True)\n",
    "\n",
    "inner_df = pd.DataFrame()\n",
    "inner_df[\"Composition\"] = comp_test\n",
    "inner_df[\"Experimental\"] = test_labels\n",
    "inner_df[\"Predictions\"] = test_pred\n",
    "inner_df[\"residual\"] = test_pred - test_labels\n",
    "inner_df[\"STD\"] = test_std\n",
    "inner_df = inner_df.sort_values(by='Experimental')\n",
    "display(inner_df)\n",
    "\n",
    "mean_rf = mean_absolute_error(test_labels, test_pred)\n",
    "print(mean_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.errorbar(all_labels, all_pred, color='green', marker='o', markersize=12, linestyle='None', label='Training Data')\n",
    "plt.errorbar(test_labels, test_pred, yerr=test_std, color='red', marker='o', markersize=12, linestyle='None',label='Testing Data')\n",
    "plt.plot([-1, 20], [-1, 20], linestyle='dashed', color='black')\n",
    "plt.xticks(np.linspace(0,20,11),fontsize=26)\n",
    "plt.yticks(np.linspace(0,20,11), fontsize=26)\n",
    "plt.xlim([-1,20])\n",
    "plt.ylim([-1,20])\n",
    "plt.grid()\n",
    "plt.legend(loc=2, fontsize=22)\n",
    "plt.ylabel('Predicted Conductivity x10$^{-4}$ (S/cm)', fontsize=26)\n",
    "plt.xlabel('Experimental Conductivity x10$^{-4}$ (S/cm)', fontsize=26)\n",
    "plt.title('Random Forest', fontsize=26)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ta Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ta_line_comps = []\n",
    "ta_array = np.linspace(0,1,101) # X-Axis granularity to generate nominal stoichometries\n",
    "x_values_pred = np.linspace(0,1,101) # X-Axis granulatirty for plotting\n",
    "\n",
    "\n",
    "# Generating stoichiometric nominal compositions doped with Ta\n",
    "for stg in ta_array:\n",
    "    base_equation_string = \"Li\" + (('%f'%(np.around(7 - stg, decimals=3, out=None))).rstrip('0').rstrip('.')) + \"La3\" + \"Zr\" + (('%f'%(np.around(2 - stg, decimals=3, out=None))).rstrip('0').rstrip('.')) + (\"Ta\" + (('%f'%(np.around(stg, decimals=3, out=None))).rstrip('0').rstrip('.')) if stg > 0 else '') + \"O12\"\n",
    "    ta_line_comps.append(base_equation_string)\n",
    "\n",
    "\n",
    "# Getting descriptors from the chemical formula\n",
    "    \n",
    "ta_composition_test_set_dataframe = pd.DataFrame(ta_line_comps, columns=['chemicalFormula'])\n",
    "ta_composition_test_set_dataframe['composition'] = ta_composition_test_set_dataframe['chemicalFormula'].apply(get_composition) # Transformation of chemicalformula string into Matminer composition\n",
    "ta_feat_test_set= np.array(f.featurize_many(ta_composition_test_set_dataframe['composition'], ignore_errors=True)) # Array to store such features\n",
    "\n",
    "ta_temp_array = np.array([25.0 for _ in range(ta_feat_test_set.shape[0])]).reshape(-1,1) # Array of simulated measuring temperatures\n",
    "ta_feat_test_set = np.hstack((ta_feat_test_set, ta_temp_array))\n",
    "\n",
    "\n",
    "# We need to drop the same columns that were dropped from the original training set so that this map has the same number of descriptors\n",
    "\n",
    "# This code is to drop columns with std = 0. \n",
    "ta_parsed_features_test_set = pd.DataFrame(ta_feat_test_set)\n",
    "ta_parsed_features_test_set_2 = ta_parsed_features_test_set.loc[:,  x_df_prior.std() != 0] # Dropping same columns that were dropped on the training data\n",
    "\n",
    "# Turning these values into an array\n",
    "ta_values = [list(ta_parsed_features_test_set_2.iloc[x]) for x in range(len(ta_parsed_features_test_set_2.index))]\n",
    "ta_values = np.array(ta_values, dtype = float) \n",
    "\n",
    "\n",
    "# Making predictions\n",
    "y_values_pred, err_values_pred = randomforest_model.predict(ta_values, return_std=True)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize =(10,10))\n",
    "plt.plot(x_values_exp,y_values_exp, color='blue', marker='o', fillstyle='none', markersize=12, linestyle = \"None\",  label = \"Experiment\") # This is from the list of Ta-compositions before dropping duplicates\n",
    "plt.plot(x_values_dupmed,y_values_dupmed, color='black',  marker='x', markersize=14, linestyle = \"None\", label = \"Median\") # This is from the list of Ta-compositions after dropping duplicates\n",
    "\n",
    "plt.plot(x_values_pred,y_values_pred, color='red', linestyle='solid',label='Prediction') # Predictions\n",
    "plt.fill_between(np.array(x_values_pred), np.array(y_values_pred)-np.array(err_values_pred), np.array(y_values_pred)+np.array(err_values_pred), facecolor='#EBECF0') # Uncertainty regions\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(fontsize=22)\n",
    "plt.xlabel(\"Ta Content\", fontsize =26)\n",
    "plt.ylabel(\"Ionic Conductivity x10$^{-4}$ (S/cm)\", fontsize =26)\n",
    "plt.xticks(fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Active Learning Approach\n",
    "\n",
    "Active learning is the use of algorithms not for regression, but for the improvement of the input sample space that guides the 'experiments' required to get to such maximum values. Even if it does make predictions for a specific material, its main task is the selection of the most likely candidate to be in a global maxima. This is the approach introduced in the paper by Julia Ling et al.\n",
    "<br>\n",
    "<br>\n",
    "We will select an initial set of 10 entries, and we'll make sure the highest value is not in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_values.copy()\n",
    "y = all_labels.copy()\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "entry_number_init = 10\n",
    "\n",
    "in_train = np.zeros(len(data), dtype=np.bool)\n",
    "in_train[np.random.choice(len(data), entry_number_init, replace=False)] = True\n",
    "print('Picked {} training entries'.format(in_train.sum()))\n",
    "assert not np.isclose(max(y), max(y[in_train]))\n",
    "print(max(y[in_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then train the model with this initial set and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X[in_train], y[in_train])\n",
    "y_pred, y_std = model.predict(X[~in_train], return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this approach, we will be querying the next material to sample using four different acquisition functions:\n",
    "<center>\n",
    "    \n",
    "### MEI\n",
    "\n",
    "<br>\n",
    "$$ u(x) = max(f(x)_{pred} - f(x)_{max train})$$\n",
    "<br>\n",
    "\n",
    "### MLI\n",
    "\n",
    "<br>\n",
    "$$ u(x) = \\frac{max(f(x)_{pred} - f(x)_{max train})}  {\\sigma} $$\n",
    "<br>\n",
    "\n",
    "### MU\n",
    "<br>\n",
    "$$ u(x) = max(\\sigma)$$\n",
    "<br>\n",
    "\n",
    "### UCB\n",
    "<br>\n",
    "$$ u(x) =  f(x)_{pred} + K * \\sigma $$\n",
    "<br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mei_selection = np.argmax(y_pred)\n",
    "mli_selection = np.argmax(np.divide(y_pred - np.max(y[in_train]), y_std))\n",
    "mu_selection = np.argmax(y_std)\n",
    "ucb_selection = np.argmax([sum(x) for x in zip(y_pred, y_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted ' + property_interest + ' of material #{} selected based on MEI: {:.6f} +/- {:.6f}'.format(mei_selection, y_pred[mei_selection], y_std[mei_selection]))\n",
    "print('Predicted ' + property_interest + ' of material #{} selected based on MLI: {:.6f} +/- {:.6f}'.format(mli_selection, y_pred[mli_selection], y_std[mli_selection]))\n",
    "print('Predicted ' + property_interest + ' of material #{} selected based on MU: {:.6f} +/- {:.6f}'.format(mu_selection, y_pred[mu_selection], y_std[mu_selection]))\n",
    "print('Predicted ' + property_interest + ' of material #{} selected based on UCB: {:.6f} +/- {:.6f}'.format(ucb_selection, y_pred[ucb_selection], y_std[ucb_selection]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the approaches. We start with the initial set and we run all experiments to test if we can get to the sample with the highest value. Each of the approaches selects a different next point to query and include in the training set. This approach allow for us to track the entire run of the experiments. The following cell takes about 2-3 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 90\n",
    "all_inds = set(range(len(y)))\n",
    "\n",
    "random_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "mei_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "mli_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "mu_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "ucb_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "random_train_inds = []\n",
    "mei_train_inds = []\n",
    "mli_train_inds = []\n",
    "ucb_train_inds = []\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "\n",
    "    # RANDOM\n",
    "    \n",
    "    random_train_inds = random_train[-1].copy()\n",
    "    \n",
    "    random_search_inds = list(all_inds.difference(random_train_inds))\n",
    "    \n",
    "    model.fit(X[random_train_inds], y[random_train_inds])\n",
    "    random_y_pred = model.predict(X[random_search_inds])\n",
    "    \n",
    "    random_train_inds.append(np.random.choice(random_search_inds))\n",
    "    random_train.append(random_train_inds)\n",
    "    \n",
    "    # Maximum Expected Improvement\n",
    "    \n",
    "    mei_train_inds = mei_train[-1].copy()    \n",
    "    mei_search_inds = list(all_inds.difference(mei_train_inds))\n",
    "    \n",
    "    # Pick entry with the largest maximum value\n",
    "    model.fit(X[mei_train_inds], y[mei_train_inds])\n",
    "    mei_y_pred = model.predict(X[mei_search_inds])\n",
    "\n",
    "    mei_train_inds.append(mei_search_inds[np.argmax(mei_y_pred)])\n",
    "    mei_train.append(mei_train_inds)\n",
    "\n",
    "    # Maximum Likelihood of Improvement\n",
    "    \n",
    "    mli_train_inds = mli_train[-1].copy()  # Last iteration\n",
    "    mli_search_inds = list(all_inds.difference(mli_train_inds))\n",
    "    \n",
    "    # Pick entry with the largest maximum value\n",
    "    model.fit(X[mli_train_inds], y[mli_train_inds])\n",
    "    mli_y_pred, mli_y_std = model.predict(X[mli_search_inds], return_std=True)\n",
    "    mli_train_inds.append(mli_search_inds[np.argmax(np.divide(mli_y_pred - np.max(y[mli_train_inds]), mli_y_std))])\n",
    "    mli_train.append(mli_train_inds)\n",
    "    \n",
    "    # Maximum Uncertainty\n",
    "    \n",
    "    mu_train_inds = mu_train[-1].copy()  # Last iteration\n",
    "    mu_search_inds = list(all_inds.difference(mu_train_inds))\n",
    "    \n",
    "    # Pick entry with the largest maximum value\n",
    "    model.fit(X[mu_train_inds], y[mu_train_inds])\n",
    "    mu_y_pred, mu_y_std = model.predict(X[mu_search_inds], return_std=True)\n",
    "    mu_train_inds.append(mu_search_inds[np.argmax(mu_y_std)])\n",
    "    mu_train.append(mu_train_inds)\n",
    "    \n",
    "    # Upper Conf Bound\n",
    "    \n",
    "    ucb_train_inds = ucb_train[-1].copy()  # Last iteration\n",
    "    ucb_search_inds = list(all_inds.difference(ucb_train_inds))\n",
    "    \n",
    "    # Pick entry with the largest maximum value\n",
    "    model.fit(X[ucb_train_inds], y[ucb_train_inds])\n",
    "    ucb_y_pred, ucb_y_std = model.predict(X[ucb_search_inds], return_std=True)\n",
    "    ucb_train_inds.append(ucb_search_inds[np.argmax([sum(x) for x in zip(ucb_y_pred, ucb_y_std)])])\n",
    "    ucb_train.append(ucb_train_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present a static version of the plot from the main notebook, for publication purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize=(18,14))\n",
    "ax = ax.flatten()\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.4)\n",
    "\n",
    "# FIRST PLOT, Approach lines\n",
    "\n",
    "random_line, = ax[0].plot([], [], color='green', label='Random')\n",
    "mei_line, = ax[0].plot([], [], color='blue', label='MEI')\n",
    "mli_line, = ax[0].plot([], [], color='red', label='MLI')\n",
    "mu_line, = ax[0].plot([], [], color='purple', label='MU')\n",
    "ucb_line, = ax[0].plot([], [], color='orange', label='UCB')\n",
    "max_line, = ax[0].plot(range(n_steps), [max(y) for m in range(n_steps)], '--', color='black', label='Maximum Value')\n",
    "\n",
    "random_chk, = ax[0].plot([], [], markersize=10, marker='*', linestyle='None', color='green')\n",
    "mei_chk, = ax[0].plot([], [], markersize=10, marker='*', linestyle='None', color='blue')\n",
    "mli_chk, = ax[0].plot([], [], markersize=10, marker='*', linestyle='None', color='red')\n",
    "mu_chk, = ax[0].plot([], [], markersize=10, marker='*', linestyle='None', color='purple')\n",
    "ucb_chk, = ax[0].plot([], [], markersize=10, marker='*', linestyle='None', color='orange')\n",
    "\n",
    "# ax0leg = ax[0].legend(loc=4, prop={'size': 12})\n",
    "# ax0leg.get_frame().set_edgecolor('k')\n",
    "\n",
    "\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"Number of Experiments\", fontsize=24)\n",
    "ax[0].set_ylabel(\"Maximum \"+ property_interest, fontsize=24)\n",
    "mli_line.axes.axis([0, n_steps-1, 0, 1.1*max(y)])\n",
    "\n",
    "mli_line.axes.get_yaxis().set_tick_params(labelsize=20)\n",
    "mli_line.axes.get_xaxis().set_tick_params(labelsize=20)\n",
    "\n",
    "\n",
    "mei_line.axes.get_yaxis().set_tick_params(labelsize=20)\n",
    "mu_line.axes.get_yaxis().set_tick_params(labelsize=20)\n",
    "random_line.axes.get_yaxis().set_tick_params(labelsize=20)\n",
    "ucb_line.axes.get_yaxis().set_tick_params(labelsize=20)\n",
    "\n",
    "# SECOND PLOT, Random Prediction\n",
    "\n",
    "all_values_samples = ax[5].plot(list(all_inds), y, marker='o', alpha=0.2, color='gray', linestyle='None', markersize=10, label='Values')\n",
    "\n",
    "random_reallabel = [y[index] for index in random_train_inds]\n",
    "\n",
    "random_initial_set = ax[5].plot(random_train_inds[:entry_number_init], random_reallabel[:entry_number_init], color='black', marker='o', linestyle= 'None',  markersize=10, label = 'Initial Set')\n",
    "\n",
    "\n",
    "# ax5leg = ax[5].legend(prop={'size': 12})\n",
    "# ax5leg.get_frame().set_edgecolor('k')\n",
    "\n",
    "ax[5].grid()\n",
    "ax[5].set_title(\"Random\", fontsize=26)\n",
    "ax[5].set_xlabel(\"Test Candidates\", fontsize=24)\n",
    "ax[5].set_ylabel(property_interest, fontsize=24)\n",
    "\n",
    "\n",
    "#THIRD PLOT, MEI Prediction\n",
    "\n",
    "all_values_samples = ax[1].plot(list(all_inds), y, marker='o', alpha=0.2, color='gray', linestyle='None', markersize=10, label='Values')\n",
    "\n",
    "mei_reallabel = [y[index] for index in mei_train_inds]\n",
    "\n",
    "mei_initial_set = ax[1].plot(mei_train_inds[:entry_number_init], mei_reallabel[:entry_number_init], color='black', marker='o',linestyle= 'None',   markersize=10, label = 'Initial Set')\n",
    "\n",
    "# ax1leg = ax[1].legend(prop={'size': 12})\n",
    "# ax1leg.get_frame().set_edgecolor('k')\n",
    "\n",
    "ax[1].grid()\n",
    "ax[1].set_title(\"Maximum Expected \\n Improvement (MEI)\", fontsize=26)\n",
    "ax[1].set_xlabel(\"Test Candidates\", fontsize=24)\n",
    "ax[1].set_ylabel(property_interest, fontsize=24)\n",
    "\n",
    "# 4th PLOT, MLI Prediction\n",
    "\n",
    "all_values_samples = ax[3].plot(list(all_inds), y, marker='o', alpha=0.2, color='gray', linestyle='None', markersize=10, label='Values')\n",
    "\n",
    "mli_reallabel = [y[index] for index in mli_train_inds]\n",
    "\n",
    "mli_initial_set = ax[3].plot(mli_train_inds[:entry_number_init], mli_reallabel[:entry_number_init], color='black', marker='o', linestyle= 'None',  markersize=10, label = 'Initial Set')\n",
    "\n",
    "# ax3leg = ax[3].legend(prop={'size': 12})\n",
    "# ax3leg.get_frame().set_edgecolor('k')\n",
    "\n",
    "ax[3].grid()\n",
    "ax[3].set_title(\"Maximum Likelihood \\n of Improvement (MLI)\", fontsize=26)\n",
    "ax[3].set_xlabel(\"Test Candidates\", fontsize=24)\n",
    "ax[3].set_ylabel(property_interest, fontsize=24)\n",
    "\n",
    "\n",
    "# 5th plot, MU Prediction\n",
    "\n",
    "all_values_samples = ax[4].plot(list(all_inds), y, marker='o', alpha=0.2, color='gray', linestyle='None', markersize=10, label='Values')\n",
    "\n",
    "mu_reallabel = [y[index] for index in mu_train_inds]\n",
    "\n",
    "mu_initial_set = ax[4].plot(mu_train_inds[:entry_number_init], mu_reallabel[:entry_number_init], color='black', marker='o', linestyle= 'None',  markersize=10, label = 'Initial Set')\n",
    "\n",
    "\n",
    "\n",
    "# ax4leg = ax[4].legend(prop={'size': 12})\n",
    "# ax4leg.get_frame().set_edgecolor('k')\n",
    "\n",
    "ax[4].grid()\n",
    "ax[4].set_title(\"Maximum \\n Uncertainty (MU)\", fontsize=26)\n",
    "ax[4].set_xlabel(\"Test Candidates\", fontsize=24)\n",
    "ax[4].set_ylabel(property_interest, fontsize=24)\n",
    "\n",
    "# 6th plot, UCB Prediction\n",
    "\n",
    "all_values_samples = ax[2].plot(list(all_inds), y, marker='o', alpha=0.2, color='gray', linestyle='None', markersize=10, label='Values')\n",
    "\n",
    "ucb_reallabel = [y[index] for index in ucb_train_inds]\n",
    "\n",
    "ucb_initial_set = ax[2].plot(ucb_train_inds[:entry_number_init], ucb_reallabel[:entry_number_init], color='black', marker='o', linestyle= 'None',  markersize=10, label = 'Initial Set')\n",
    "\n",
    "\n",
    "# ax2leg = ax[2].legend(prop={'size': 12})\n",
    "# ax2leg.get_frame().set_edgecolor('k')\n",
    "\n",
    "ax[2].grid()\n",
    "ax[2].set_title(\"Upper Confidence \\n Bound (UCB)\", fontsize=26)\n",
    "ax[2].set_xlabel(\"Test Candidates\", fontsize=24)\n",
    "ax[2].set_ylabel(property_interest, fontsize=24)\n",
    "\n",
    "#################################################\n",
    "\n",
    "num=90\n",
    "\n",
    "\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "if num > 0:\n",
    "\n",
    "    random_graph = [max(y[list(t)]) for t in random_train[:num]]\n",
    "    chk_index = [i for i, j in enumerate(random_graph) if j == max(y)][0]\n",
    "    random_line.set_data(np.arange(len(random_train))[:chk_index+1], [max(y[list(t)]) for t in random_train[:chk_index+1]])\n",
    "    random_chk.set_data(chk_index, max(random_graph))\n",
    "    \n",
    "    a = list(enumerate(random_train_inds[entry_number_init:entry_number_init+chk_index]))\n",
    "    a = [_[0] for _ in a]\n",
    "    \n",
    "    n = len(a)\n",
    "    colors = np.array(pl.cm.Greens(np.linspace(0,1,n)))\n",
    " \n",
    "    random_sample_real = ax[5].scatter(random_train_inds[entry_number_init:entry_number_init+chk_index], random_reallabel[entry_number_init:entry_number_init+chk_index], c=colors, marker='o', s=100,linestyle= 'None', label = 'Tests')\n",
    "    random_sample_real.axes.axis([0, len(y), 0, 1.1*max(y)])\n",
    "    random_sample_real.axes.get_xaxis().set_ticks([])\n",
    "    random_sample_real.axes.get_yaxis().set_tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    mei_graph = [max(y[list(t)]) for t in mei_train[:num]]\n",
    "    chk_index = [i for i, j in enumerate(mei_graph) if j == max(y)][0]\n",
    "    mei_line.set_data(np.arange(len(mei_train))[:chk_index+1], [max(y[list(t)]) for t in mei_train[:chk_index+1]])\n",
    "    mei_chk.set_data(chk_index, max(mei_graph))\n",
    "    \n",
    "    a = list(enumerate(mei_train_inds[entry_number_init:entry_number_init+chk_index]))\n",
    "    a = [_[0] for _ in a]\n",
    "    \n",
    "    n = len(a)\n",
    "    colors = np.array(pl.cm.Blues(np.linspace(0,1,n)))    \n",
    "    \n",
    "    mei_sample_real = ax[1].scatter(mei_train_inds[entry_number_init:entry_number_init+chk_index], mei_reallabel[entry_number_init:entry_number_init+chk_index], c=colors, marker='o', s=100,linestyle= 'None', label = 'Tests')\n",
    "    mei_sample_real.axes.axis([0, len(y), 0, 1.1*max(y)])\n",
    "    mei_sample_real.axes.get_xaxis().set_ticks([])\n",
    "    mei_sample_real.axes.get_yaxis().set_tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "\n",
    "    mli_graph = [max(y[list(t)]) for t in mli_train[:num]]\n",
    "    chk_index = [i for i, j in enumerate(mli_graph) if j == max(y)][0]\n",
    "    mli_line.set_data(np.arange(len(mli_train))[:chk_index+1], [max(y[list(t)]) for t in mli_train[:chk_index+1]])\n",
    "    mli_chk.set_data(chk_index, max(mli_graph))\n",
    "\n",
    "    \n",
    "    a = list(enumerate(mli_train_inds[entry_number_init:entry_number_init+chk_index]))\n",
    "    a = [_[0] for _ in a]\n",
    "    \n",
    "    n = len(a)\n",
    "    colors = np.array(pl.cm.Reds(np.linspace(0,1,n)))    \n",
    "    \n",
    "    mli_sample_real = ax[3].scatter(mli_train_inds[entry_number_init:entry_number_init+chk_index], mli_reallabel[entry_number_init:entry_number_init+chk_index], c=colors, marker='o', s=100,linestyle= 'None', label = 'Tests') \n",
    "    mli_sample_real.axes.axis([0, len(y), 0, 1.1*max(y)])\n",
    "    mli_sample_real.axes.get_xaxis().set_ticks([])\n",
    "    mli_sample_real.axes.get_yaxis().set_tick_params(labelsize=20)    \n",
    "    \n",
    "\n",
    "    mu_graph = [max(y[list(t)]) for t in mu_train[:num]]\n",
    "    chk_index = [i for i, j in enumerate(mu_graph) if j == max(y)][0]\n",
    "    mu_line.set_data(np.arange(len(mu_train))[:chk_index+1], [max(y[list(t)]) for t in mu_train[:chk_index+1]])\n",
    "    mu_chk.set_data(chk_index, max(mu_graph))\n",
    "    \n",
    "    a = list(enumerate(mu_train_inds[entry_number_init:entry_number_init+chk_index]))\n",
    "    a = [_[0] for _ in a]\n",
    "    \n",
    "    n = len(a)\n",
    "    colors = np.array(pl.cm.Purples(np.linspace(0,1,n)))     \n",
    "    \n",
    "    mu_sample_real = ax[4].scatter(mu_train_inds[entry_number_init:entry_number_init+chk_index], mu_reallabel[entry_number_init:entry_number_init+chk_index], c=colors, marker='o', s=100,linestyle= 'None', label = 'Tests') \n",
    "    mu_sample_real.axes.axis([0, len(y), 0, 1.1*max(y)])\n",
    "    mu_sample_real.axes.get_xaxis().set_ticks([])\n",
    "    mu_sample_real.axes.get_yaxis().set_tick_params(labelsize=20)    \n",
    "    \n",
    "    \n",
    "    ucb_graph = [max(y[list(t)]) for t in ucb_train[:num]]\n",
    "    chk_index = [i for i, j in enumerate(ucb_graph) if j == max(y)][0]\n",
    "    ucb_line.set_data(np.arange(len(ucb_train))[:chk_index+1], [max(y[list(t)]) for t in ucb_train[:chk_index+1]])\n",
    "    ucb_chk.set_data(chk_index, max(ucb_graph))\n",
    "    \n",
    "    \n",
    "    a = list(enumerate(ucb_train_inds[entry_number_init:entry_number_init+chk_index]))\n",
    "    a = [_[0] for _ in a]\n",
    "    \n",
    "    n = len(a)\n",
    "    colors = np.array(pl.cm.YlOrBr(np.linspace(0,1,n)))     \n",
    "    \n",
    "    ucb_sample_real = ax[2].scatter(ucb_train_inds[entry_number_init:entry_number_init+chk_index], ucb_reallabel[entry_number_init:entry_number_init+chk_index], c=colors, marker='o', s=100,linestyle= 'None', label = 'Tests') \n",
    "    ucb_sample_real.axes.axis([0, len(y), 0, 1.1*max(y)])\n",
    "    ucb_sample_real.axes.get_xaxis().set_ticks([])\n",
    "    ucb_sample_real.axes.get_yaxis().set_tick_params(labelsize=20)    \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 30-Trial Run using the OLDEST data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_values.copy()\n",
    "y = all_labels.copy()\n",
    "\n",
    "entry_number_init = 10\n",
    "in_train = np.zeros(len(data), dtype=np.bool)\n",
    "oldest = np.argpartition(years, entry_number_init)\n",
    "display(data.loc[oldest,['chemicalFormula', 'Year Published']].head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_trials = []\n",
    "mei_trials = []\n",
    "mli_trials = []\n",
    "ucb_trials = []\n",
    "mu_trials = []\n",
    "\n",
    "np.random.seed(2) # Random Seed\n",
    "\n",
    "trial = 0\n",
    "\n",
    "while trial < 30:\n",
    "    \n",
    "    model = RandomForestRegressor(num_trees=500)\n",
    "\n",
    "    entry_number_init = 10\n",
    "    \n",
    "    # -----\n",
    "    \n",
    "    in_train = np.zeros(len(data), dtype=np.bool)\n",
    "    oldest = np.argpartition(years, entry_number_init)\n",
    "    in_train[oldest[:entry_number_init]] = True\n",
    "    print('Picked {} training entries'.format(in_train.sum()))\n",
    "\n",
    "    # -----    \n",
    "    \n",
    "#     in_train = np.zeros(len(data), dtype=np.bool)\n",
    "#     in_train[np.random.choice(len(data), entry_number_init, replace=False)] = True\n",
    "#     print('Picked {} training entries'.format(in_train.sum()))\n",
    "    \n",
    "    if not (np.isclose(max(y), max(y[in_train]))):\n",
    "        trial += 1\n",
    "    else: \n",
    "        continue\n",
    "\n",
    "    model.fit(X[in_train], y[in_train])\n",
    "    y_pred, y_std = model.predict(X[~in_train], return_std=True)\n",
    "\n",
    "    print(trial)\n",
    "    n_steps = 90\n",
    "    all_inds = set(range(len(y)))\n",
    "\n",
    "    random_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mei_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mli_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mu_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    ucb_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    random_train_inds = []\n",
    "    mei_train_inds = []\n",
    "    mli_train_inds = []\n",
    "    ucb_train_inds = []\n",
    "\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        # RANDOM\n",
    "\n",
    "        random_train_inds = random_train[-1].copy()\n",
    "\n",
    "        random_search_inds = list(all_inds.difference(random_train_inds))\n",
    "\n",
    "        model.fit(X[random_train_inds], y[random_train_inds])\n",
    "        random_y_pred = model.predict(X[random_search_inds])\n",
    "\n",
    "        random_train_inds.append(np.random.choice(random_search_inds))\n",
    "        random_train.append(random_train_inds)\n",
    "\n",
    "        # Maximum Expected Improvement\n",
    "\n",
    "        mei_train_inds = mei_train[-1].copy()    \n",
    "        mei_search_inds = list(all_inds.difference(mei_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[mei_train_inds], y[mei_train_inds])\n",
    "        mei_y_pred = model.predict(X[mei_search_inds])\n",
    "\n",
    "        mei_train_inds.append(mei_search_inds[np.argmax(mei_y_pred)])\n",
    "        mei_train.append(mei_train_inds)\n",
    "\n",
    "        # Maximum Likelihood of Improvement\n",
    "\n",
    "        mli_train_inds = mli_train[-1].copy()  # Last iteration\n",
    "        mli_search_inds = list(all_inds.difference(mli_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[mli_train_inds], y[mli_train_inds])\n",
    "        mli_y_pred, mli_y_std = model.predict(X[mli_search_inds], return_std=True)\n",
    "        mli_train_inds.append(mli_search_inds[np.argmax(np.divide(mli_y_pred - np.max(y[mli_train_inds]), mli_y_std))])\n",
    "        mli_train.append(mli_train_inds)\n",
    "\n",
    "        # Maximum Uncertainty\n",
    "\n",
    "        mu_train_inds = mu_train[-1].copy()  # Last iteration\n",
    "        mu_search_inds = list(all_inds.difference(mu_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[mu_train_inds], y[mu_train_inds])\n",
    "        mu_y_pred, mu_y_std = model.predict(X[mu_search_inds], return_std=True)\n",
    "        mu_train_inds.append(mu_search_inds[np.argmax(mu_y_std)])\n",
    "        mu_train.append(mu_train_inds)\n",
    "\n",
    "        # Upper Conf Bound\n",
    "\n",
    "        ucb_train_inds = ucb_train[-1].copy()  # Last iteration\n",
    "        ucb_search_inds = list(all_inds.difference(ucb_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[ucb_train_inds], y[ucb_train_inds])\n",
    "        ucb_y_pred, ucb_y_std = model.predict(X[ucb_search_inds], return_std=True)\n",
    "        ucb_train_inds.append(ucb_search_inds[np.argmax([sum(x) for x in zip(ucb_y_pred, ucb_y_std)])])\n",
    "        ucb_train.append(ucb_train_inds)\n",
    "\n",
    "    if np.max(y[random_train_inds]) == np.max(y):\n",
    "        random_trials.append(np.argmax(y[random_train_inds][10:])+1)\n",
    "    else:\n",
    "        random_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[mei_train_inds]) == np.max(y):\n",
    "        mei_trials.append(np.argmax(y[mei_train_inds][10:])+1)\n",
    "    else:\n",
    "        mei_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[mli_train_inds]) == np.max(y):\n",
    "        mli_trials.append(np.argmax(y[mli_train_inds][10:])+1)\n",
    "    else:\n",
    "        mli_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[ucb_train_inds]) == np.max(y):\n",
    "        ucb_trials.append(np.argmax(y[ucb_train_inds][10:])+1)\n",
    "    else:\n",
    "        ucb_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[mu_train_inds]) == np.max(y):\n",
    "        mu_trials.append(np.argmax(y[mu_train_inds][10:])+1)\n",
    "    else:\n",
    "        mu_trials.append(n_steps)\n",
    "        \n",
    "print(\"Random\", random_trials)\n",
    "print(\"MEI\", mei_trials)\n",
    "print(\"MLI\", mli_trials)\n",
    "print(\"UCB\", ucb_trials)\n",
    "print(\"MU\", mu_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Values are listed explicitly here because the previous cell takes a lot of time to run (30 minutes). You can comment these out and work with the variables directly from the cell above.\n",
    "\n",
    "random_trials = [17, 39, 23, 41, 65, 23, 65, 32, 62, 36, 14, 76, 11, 59, 56, 79, 50, 33, 61, 19, 81, 46, 51, 71, 69, 6, 79, 39, 5, 6]\n",
    "mei_trials =  [13, 11, 13, 13, 9, 12, 9, 11, 12, 8, 13, 12, 11, 8, 5, 13, 13, 13, 12, 9, 9, 13, 13, 13, 9, 13, 8, 16, 16, 9]\n",
    "mli_trials =  [3, 12, 2, 17, 12, 12, 3, 13, 3, 12, 17, 13, 3, 14, 12, 12, 3, 15, 16, 13, 3, 11, 14, 17, 3, 15, 4, 13, 3, 12]\n",
    "ucb_trials =  [14, 14, 4, 13, 3, 10, 13, 12, 4, 10, 4, 11, 4, 14, 11, 15, 10, 14, 11, 15, 15, 10, 4, 11, 4, 14, 10, 14, 14, 11]\n",
    "mu_trials = [9, 17, 24, 24, 33, 29, 27, 10, 30, 45, 28, 25, 31, 11, 33, 26, 9, 23, 25, 28, 24, 29, 31, 25, 31, 23, 11, 25, 22, 25]\n",
    "\n",
    "objects = ('MLI', 'UCB', 'MEI', 'MU', 'Random')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "performance = [np.mean(mli_trials),np.mean(ucb_trials),np.mean(mei_trials),np.mean(mu_trials),np.mean(random_trials)]\n",
    "uncertainty = [np.std(mli_trials),np.std(ucb_trials), np.std(mei_trials), np.std(mu_trials),np.std(random_trials)]\n",
    "uncertainty_adj = [x/np.sqrt(30) for x in uncertainty]\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.bar(y_pos, performance, yerr=uncertainty_adj, color=['red', 'orange', 'blue', 'purple', 'green'], align='center', alpha=0.5)\n",
    "plt.axhline(y=45 , linestyle= 'dashed' , linewidth=3, color='gray')\n",
    "plt.xticks(y_pos, objects, fontsize=28)\n",
    "plt.yticks(fontsize=28)\n",
    "plt.ylim([0,60])\n",
    "plt.ylabel('Number of Experiments', fontsize=28)\n",
    "#plt.title('Information Acquistion Functions', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 30-Trial Run using the RANDOM starting data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_trials = []\n",
    "mei_trials = []\n",
    "mli_trials = []\n",
    "ucb_trials = []\n",
    "mu_trials = []\n",
    "\n",
    "np.random.seed(2) # Random Seed\n",
    "\n",
    "trial = 0\n",
    "\n",
    "while trial < 30:\n",
    "    \n",
    "    model = RandomForestRegressor(num_trees=200)\n",
    "\n",
    "    entry_number_init = 10\n",
    "    \n",
    "    # -----\n",
    "    \n",
    "#     in_train = np.zeros(len(data), dtype=np.bool)\n",
    "#     oldest = np.argpartition(years, entry_number_init)\n",
    "#     in_train[oldest[:entry_number_init]] = True\n",
    "#     print('Picked {} training entries'.format(in_train.sum()))\n",
    "\n",
    "    # -----    \n",
    "    \n",
    "    in_train = np.zeros(len(data), dtype=np.bool)\n",
    "    in_train[np.random.choice(len(data), entry_number_init, replace=False)] = True\n",
    "    print('Picked {} training entries'.format(in_train.sum()))\n",
    "    \n",
    "    if not (np.isclose(max(y), max(y[in_train]))):\n",
    "        trial += 1\n",
    "    else: \n",
    "        continue\n",
    "\n",
    "    model.fit(X[in_train], y[in_train])\n",
    "    y_pred, y_std = model.predict(X[~in_train], return_std=True)\n",
    "\n",
    "    print(trial)\n",
    "    n_steps = 90\n",
    "    all_inds = set(range(len(y)))\n",
    "\n",
    "    random_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mei_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mli_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    mu_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    ucb_train = [list(set(np.where(in_train)[0].tolist()))]\n",
    "    \n",
    "    print(\"Initial Set:\", random_train[0])\n",
    "    \n",
    "    random_train_inds = []\n",
    "    mei_train_inds = []\n",
    "    mli_train_inds = []\n",
    "    ucb_train_inds = []\n",
    "\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        # RANDOM\n",
    "\n",
    "        random_train_inds = random_train[-1].copy()\n",
    "\n",
    "        random_search_inds = list(all_inds.difference(random_train_inds))\n",
    "\n",
    "        model.fit(X[random_train_inds], y[random_train_inds])\n",
    "        random_y_pred = model.predict(X[random_search_inds])\n",
    "\n",
    "        random_train_inds.append(np.random.choice(random_search_inds))\n",
    "        random_train.append(random_train_inds)\n",
    "\n",
    "        # Maximum Expected Improvement\n",
    "\n",
    "        mei_train_inds = mei_train[-1].copy()    \n",
    "        mei_search_inds = list(all_inds.difference(mei_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[mei_train_inds], y[mei_train_inds])\n",
    "        mei_y_pred = model.predict(X[mei_search_inds])\n",
    "\n",
    "        mei_train_inds.append(mei_search_inds[np.argmax(mei_y_pred)])\n",
    "        mei_train.append(mei_train_inds)\n",
    "\n",
    "        # Maximum Likelihood of Improvement\n",
    "\n",
    "        mli_train_inds = mli_train[-1].copy()  # Last iteration\n",
    "        mli_search_inds = list(all_inds.difference(mli_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[mli_train_inds], y[mli_train_inds])\n",
    "        mli_y_pred, mli_y_std = model.predict(X[mli_search_inds], return_std=True)\n",
    "        mli_train_inds.append(mli_search_inds[np.argmax(np.divide(mli_y_pred - np.max(y[mli_train_inds]), mli_y_std))])\n",
    "        mli_train.append(mli_train_inds)\n",
    "\n",
    "        # Maximum Uncertainty\n",
    "\n",
    "        mu_train_inds = mu_train[-1].copy()  # Last iteration\n",
    "        mu_search_inds = list(all_inds.difference(mu_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[mu_train_inds], y[mu_train_inds])\n",
    "        mu_y_pred, mu_y_std = model.predict(X[mu_search_inds], return_std=True)\n",
    "        mu_train_inds.append(mu_search_inds[np.argmax(mu_y_std)])\n",
    "        mu_train.append(mu_train_inds)\n",
    "\n",
    "        # Upper Conf Bound\n",
    "\n",
    "        ucb_train_inds = ucb_train[-1].copy()  # Last iteration\n",
    "        ucb_search_inds = list(all_inds.difference(ucb_train_inds))\n",
    "\n",
    "        # Pick entry with the largest maximum value\n",
    "        model.fit(X[ucb_train_inds], y[ucb_train_inds])\n",
    "        ucb_y_pred, ucb_y_std = model.predict(X[ucb_search_inds], return_std=True)\n",
    "        ucb_train_inds.append(ucb_search_inds[np.argmax([sum(x) for x in zip(ucb_y_pred, ucb_y_std)])])\n",
    "        ucb_train.append(ucb_train_inds)\n",
    "\n",
    "    if np.max(y[random_train_inds]) == np.max(y):\n",
    "        random_trials.append(np.argmax(y[random_train_inds][10:])+1)\n",
    "    else:\n",
    "        random_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[mei_train_inds]) == np.max(y):\n",
    "        mei_trials.append(np.argmax(y[mei_train_inds][10:])+1)\n",
    "    else:\n",
    "        mei_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[mli_train_inds]) == np.max(y):\n",
    "        mli_trials.append(np.argmax(y[mli_train_inds][10:])+1)\n",
    "    else:\n",
    "        mli_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[ucb_train_inds]) == np.max(y):\n",
    "        ucb_trials.append(np.argmax(y[ucb_train_inds][10:])+1)\n",
    "    else:\n",
    "        ucb_trials.append(n_steps)\n",
    "\n",
    "    if np.max(y[mu_train_inds]) == np.max(y):\n",
    "        mu_trials.append(np.argmax(y[mu_train_inds][10:])+1)\n",
    "    else:\n",
    "        mu_trials.append(n_steps)\n",
    "        \n",
    "print(\"Random\", random_trials)\n",
    "print(\"MEI\", mei_trials)\n",
    "print(\"MLI\", mli_trials)\n",
    "print(\"UCB\", ucb_trials)\n",
    "print(\"MU\", mu_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Values are listed explicitly here because the previous cell takes a lot of time to run (30 minutes). You can comment these out and work with the variables directly from the cell above.\n",
    "\n",
    "random_trials =  [11, 12, 26, 36, 57, 60, 3, 65, 81, 83, 55, 90, 54, 68, 36, 87, 52, 74, 12, 42, 62, 44, 29, 68, 52, 8, 74, 15, 38, 15]\n",
    "mei_trials =  [8, 9, 13, 7, 4, 9, 9, 6, 14, 20, 1, 14, 28, 4, 9, 2, 13, 9, 10, 15, 11, 4, 11, 6, 10, 18, 5, 2, 10, 18]\n",
    "mli_trials = [12, 6, 22, 13, 3, 14, 23, 1, 15, 30, 2, 7, 14, 15, 19, 1, 5, 4, 11, 11, 11, 4, 16, 8, 12, 16, 11, 1, 4, 9]\n",
    "ucb_trials =  [1, 8, 21, 9, 3, 13, 23, 1, 12, 28, 2, 4, 16, 2, 13, 1, 10, 12, 10, 11, 9, 12, 14, 4, 11, 17, 14, 2, 6, 7]\n",
    "mu_trials = [11, 37, 36, 31, 20, 23, 37, 13, 3, 25, 26, 14, 33, 2, 32, 17, 22, 38, 19, 37, 10, 13, 16, 19, 10, 42, 3, 35, 38, 35]\n",
    "\n",
    "\n",
    "objects = ('MLI', 'UCB', 'MEI', 'MU', 'Random')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "performance = [np.mean(mli_trials),np.mean(ucb_trials),np.mean(mei_trials),np.mean(mu_trials),np.mean(random_trials)]\n",
    "uncertainty = [np.std(mli_trials),np.std(ucb_trials), np.std(mei_trials), np.std(mu_trials),np.std(random_trials)]\n",
    "uncertainty_adj = [x/np.sqrt(30) for x in uncertainty]\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.bar(y_pos, performance, yerr=uncertainty_adj, color=['red', 'orange', 'blue', 'purple', 'green'], align='center', alpha=0.5)\n",
    "plt.axhline(y=45 , linestyle= 'dashed' , linewidth=3, color='gray')\n",
    "plt.xticks(y_pos, objects, fontsize=28)\n",
    "plt.yticks(fontsize=28)\n",
    "plt.ylim([0,60])\n",
    "plt.ylabel('Number of Experiments', fontsize=28)\n",
    "#plt.title('Information Acquistion Functions', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Garnet Predictor for codoped LLZO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create a garnet predictor based on all of our training data. It is important to note that these predictions are for an untested region of candidates, and because of the limited data and the inability of random forests to extrapolate, should not be understood with those caveats and not taken as all encompassing predictions. Results from this figure might vary because of the inner initialization of the random forest model, but their intention is to show the limited predicted capabilities of the models by themselves, which can help us motivate the use of active learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the color-intensity plot taking the model, dopants and relevant substitutions.\n",
    "\n",
    "def prediction_surface_dualdoped(model, first_dopant, first_dopant_range, second_dopant, second_dopant_range, first_lithium_replace, first_lanthanum_replace, first_zirconium_replace, second_lithium_replace, second_lanthanum_replace, second_zirconium_replace):\n",
    "\n",
    "    # Grid of compositions\n",
    "    st_first = np.around(np.linspace(0,first_dopant_range,81), decimals=3, out=None)\n",
    "    st_second = np.around(np.linspace(0,second_dopant_range,81), decimals=3, out=None)\n",
    "\n",
    "    first_grid, second_grid = np.meshgrid(st_first, st_second)\n",
    "\n",
    "    formula_test_set = []\n",
    "    pairs = []\n",
    "\n",
    "    # Creation of nominal stoichiometries\n",
    "    \n",
    "    for stb in st_first:\n",
    "        for stg in st_second:\n",
    "            base_equation_string = \"Li\" + (('%f'%(np.around(7+first_lithium_replace*stb + second_lithium_replace*stg, decimals=3, out=None))).rstrip('0').rstrip('.')) + \"La\" + (('%f'%(np.around(3+(first_lanthanum_replace*stb) + (second_lanthanum_replace*stg), decimals=3, out=None))).rstrip('0').rstrip('.')) + \"Zr\" + (('%f'%(np.around(2+(first_zirconium_replace*stb) + (second_zirconium_replace*stg), decimals=3, out=None))).rstrip('0').rstrip('.')) + (str(first_dopant) + (('%f'%(np.around(stb, decimals=3, out=None))).rstrip('0').rstrip('.')) if stb > 0 else '') + (str(second_dopant) + (('%f'%(np.around(stg, decimals=3, out=None))).rstrip('0').rstrip('.')) if stg > 0 else '') + \"O12\"\n",
    "            formula_test_set.append(base_equation_string)\n",
    "            pairs.append((stb, stg))\n",
    "\n",
    "    # Descriptors through features\n",
    "    \n",
    "    composition_test_set_dataframe = pd.DataFrame(formula_test_set, columns=['chemicalFormula'])\n",
    "\n",
    "    composition_test_set_dataframe['composition'] = composition_test_set_dataframe['chemicalFormula'].apply(get_composition) # Transformation of chemicalformula string into Matminer composition\n",
    "    feat_test_set= np.array(f.featurize_many(composition_test_set_dataframe['composition'], ignore_errors=True)) # Array to store such features\n",
    "\n",
    "    temp_array = np.array([25.0 for _ in range(feat_test_set.shape[0])]).reshape(-1,1)\n",
    "    feat_test_set = np.hstack((feat_test_set, temp_array))\n",
    "\n",
    "    # This code is to drop columns with std = 0. \n",
    "    parsed_features_test_set = pd.DataFrame(feat_test_set)\n",
    "    parsed_features_test_set_2 = parsed_features_test_set.loc[:,  x_df_prior.std() != 0] # Dropping same columns that were dropped on the training data\n",
    "\n",
    "    values = [list(parsed_features_test_set_2.iloc[x]) for x in range(len(parsed_features_test_set_2.index))]\n",
    "    values = np.array(values, dtype = float) \n",
    "\n",
    "    # Normalization if ANNs\n",
    "    \n",
    "    if model == neuralnetwork_model:\n",
    "        values = (values - feature_mean)/ (feature_std)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = model.predict(values)#.flatten() # Prediction of the test set # Z\n",
    "    \n",
    "    \n",
    "    # Plotting\n",
    "    \n",
    "    Z = np.array([np.array(predictions.reshape(81,81)[:,x]) for x in range(81)])\n",
    "    \n",
    "    colors = []\n",
    "    \n",
    "    color_scale_colors =[[0.0, \"rgb(165,0,38)\"],\n",
    "                [0.1111111111111111, \"rgb(215,48,39)\"],\n",
    "                [0.2222222222222222, \"rgb(244,109,67)\"],\n",
    "                [0.3333333333333333, \"rgb(253,174,97)\"],\n",
    "                [0.4444444444444444, \"rgb(254,224,144)\"],\n",
    "                [0.5555555555555556, \"rgb(224,243,248)\"],\n",
    "                [0.6666666666666666, \"rgb(171,217,233)\"],\n",
    "                [0.7777777777777778, \"rgb(116,173,209)\"],\n",
    "                [0.8888888888888888, \"rgb(69,117,180)\"],\n",
    "                [1.0, \"rgb(49,54,149)\"]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    known_points = []\n",
    "    cross_test_points = []\n",
    "    \n",
    "    test_forset = inner_df[\"Composition\"].apply(get_composition)\n",
    "    \n",
    "    for _ in composition_test_set_dataframe['composition']: # Marking diamonds for the values in the entire dataset\n",
    "        if _ in list(data['composition'].values):\n",
    "            dic_ = dict(_.as_dict()) \n",
    "            if first_dopant in dic_.keys():\n",
    "                x = dic_[first_dopant]\n",
    "            else:\n",
    "                x = 0.0\n",
    "            if second_dopant in dic_.keys():\n",
    "                y = dic_[second_dopant]\n",
    "            else:\n",
    "                y = 0.0              \n",
    "            z = float(data[data['composition'] == _][\"Ionic Conductivity\"])\n",
    "            known_points.append([x,y,z])\n",
    "        if _ in list(test_forset): # Marking crosses for the values in the test dataset\n",
    "            dic_ = dict(_.as_dict()) \n",
    "            if first_dopant in dic_.keys():\n",
    "                x = dic_[first_dopant]\n",
    "            else:\n",
    "                x = 0.0\n",
    "            if second_dopant in dic_.keys():\n",
    "                y = dic_[second_dopant]\n",
    "            else:\n",
    "                y = 0.0              \n",
    "            cross_test_points.append([x,y])\n",
    "            \n",
    "    \n",
    "    known_points = np.array(known_points)   \n",
    "    known_scatter = go.Scatter(x=known_points[:,0], y=known_points[:,1], customdata=known_points[:,2], mode='markers',\n",
    "    marker=dict(\n",
    "        symbol = \"diamond\",\n",
    "        size=20,\n",
    "        color=known_points[:,2],\n",
    "        cmin = 0,\n",
    "        cmax = 18,\n",
    "        opacity=1,  colorbar=dict(thickness=20, title=dict(text='Lithium ion conductivity 10<sup> - 4</sup> S/cm',font = dict(family='Times New Roman',size=32)), titleside = 'right'), line=dict(width=2, color ='white')\n",
    "    ), hovertemplate='D1:%{x:.2f} <br>D2:%{y:.3f} <br>IC:%{customdata:.3f}')\n",
    "    \n",
    "\n",
    "    layout = go.Layout(\n",
    "        width = 800,\n",
    "        height = 600,\n",
    "        font = dict(family='Times New Roman',size=32),\n",
    "        xaxis= dict(title= first_dopant + ' content',zeroline= False, gridwidth= 2),\n",
    "        yaxis= dict(title= second_dopant + ' content',zeroline= False, gridwidth= 2),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    trace_heatmap = go.Heatmap(x=st_first, y = st_second, z=Z, showscale=False, connectgaps=True, zsmooth='best', zauto = False, zmin = 0, zmax = 18,  colorbar=dict(tickfont=dict(size=20)))\n",
    "                              \n",
    "    fig = go.Figure(data=[trace_heatmap,known_scatter], layout=layout)\n",
    "    \n",
    "\n",
    "    \n",
    "    if cross_test_points != []:\n",
    "        cross_test_points = np.array(cross_test_points)   \n",
    "        cross_test_scatter = go.Scatter(x=cross_test_points[:,0], y=cross_test_points[:,1], mode='markers',\n",
    "        marker=dict(\n",
    "            symbol = \"x\",\n",
    "            size=14,\n",
    "            color=\"white\"))\n",
    "        \n",
    "        fig.add_trace(cross_test_scatter)\n",
    "    \n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))    \n",
    "    iplot(fig)\n",
    "    \n",
    "    return first_dopant, second_dopant, st_first, st_second, pairs, colors, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_dopant, second_dopant, first_element_range, second_element_range, pairs, colors, other_predictions = prediction_surface_dualdoped(randomforest_model, \"Ta\", 1 , \"Nb\", 1, -1, 0, -1, -1, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_dopant, second_dopant, first_element_range, second_element_range, pairs, colors, other_predictions = prediction_surface_dualdoped(randomforest_model, \"Bi\", 1 , \"Ga\", 0.5, -1, 0, -1, -3, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dopant, second_dopant, first_element_range, second_element_range, pairs, colors, other_predictions = prediction_surface_dualdoped(randomforest_model, \"Sc\", 0.5, \"Ga\", 0.5 , 1, 0, -1,-3, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
