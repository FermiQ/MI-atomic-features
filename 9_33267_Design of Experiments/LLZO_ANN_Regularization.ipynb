{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms for Ionic Conductivity of LLZO-type Garnets\n",
    "\n",
    "### Authorship and credits\n",
    "\n",
    "<b> nanoHUB tools by: </b>  <i>Juan Carlos Verduzco</i> and <i>Alejandro Strachan</i>, Materials Engineering, Purdue University <br>\n",
    "<b> Database curated by: </b> <i>Juan Carlos Verduzco</i>, Materials Engineering, Purdue University <br>\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we explore regularization on our Neural Network model. This relates to our publication: \"An active learning approach for the design of doped LLZO ceramic garnets for battery applications\" (Submitted)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Querying / Processing Data <br>\n",
    "2. Obtaining features/descriptors from Matminer <br>\n",
    "3. Regression Models (Architecture 1) <br>\n",
    "    3.1 Dropuout <br>\n",
    "    3.2 L2 Regularization <br>\n",
    "    3.3 L1 Regularization  <br>\n",
    "4. Regression Models (Architecture 2) <br>\n",
    "    4.1 Dropuout <br>\n",
    "    4.2 L2 Regularization <br>\n",
    "    4.3 L1 Regularization  <br>\n",
    "\n",
    "\n",
    "Notes: This notebook uses tools from [Citrination](https://citrination.com/) and requires an account with an API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "This notebook requires several libraries to be installed. They are separated in blocks depending on their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTTING (MATPLOTLIB)\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from lolopy.learners import RandomForestRegressor\n",
    "\n",
    "# PYTHON\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# MACHINE LEARNING\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "print(keras.__version__)\n",
    "\n",
    "# CITRINATION / MATMINER\n",
    "\n",
    "from matminer.data_retrieval.retrieve_Citrine import CitrineDataRetrieval\n",
    "from matminer.featurizers.base import MultipleFeaturizer\n",
    "from matminer.featurizers import composition as cf\n",
    "from sklearn.model_selection import KFold\n",
    "from pymatgen import Composition\n",
    "from scipy.stats import norm\n",
    "\n",
    "# PLOTTING (PLOTLY)\n",
    "import plotly \n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "# This snipped refers to the adding of the CitrineKey on the main page of the tool. If you are running this notebook by itself, please comment it out and write your citrinekey in the cell below.\n",
    "file = open(os.path.expanduser('~/.citrinetools.txt'),\"r+\")\n",
    "apikey = file.readline()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Querying a Database from Citrination\n",
    "\n",
    "Matminer offers API tools to facilitate querying of databases like the Materials Project and Citrination. An individual **Citrine Key** is required for the query command <i>CitrineDataRetrieval</i>.\n",
    "\n",
    "Data is stored in a Pandas Dataframe and the list of possible properties to be queried can be consulted by setting the print_properties_options parameter to **True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/189 [00:00<?, ?it/s]/apps/share64/debian7/anaconda/anaconda-6/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n",
      "100%|██████████| 189/189 [00:04<00:00, 42.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chemicalFormula</th>\n",
       "      <th>preparation</th>\n",
       "      <th>references</th>\n",
       "      <th>Crystallographic Structure</th>\n",
       "      <th>Ionic Conductivity</th>\n",
       "      <th>Ionic Conductivity-units</th>\n",
       "      <th>Measuring Temperature</th>\n",
       "      <th>Sintering Temperature</th>\n",
       "      <th>Sintering Temperature-units</th>\n",
       "      <th>Space Group</th>\n",
       "      <th>Year Published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Li6.1La3Zr2Ga0.3O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1021/acsami.6b13902', 'doi':...</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>11.2</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>25</td>\n",
       "      <td>1100</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>Ia3d</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Li7La2Ta2Ba1O12.5</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1007/s00339-008-4494-2', 'do...</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>1.01</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>50</td>\n",
       "      <td>900</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>Ia3d</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Li6La2Nb2Ba1O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1149/1.2800764', 'doi': '10....</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>17</td>\n",
       "      <td>950</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Li6.4La3Zr2Ga0.15Al0.05O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1021/acs.chemmater.6b00579',...</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>8.8</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>20</td>\n",
       "      <td>1230</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>I43d</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Li7La3Zr2O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1021/acs.inorgchem.5b00184',...</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>1.74</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>25</td>\n",
       "      <td>900</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>Ia3d</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Li6.4La3Zr2Ga0.2O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1021/cm5008069', 'doi': '10....</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>9</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>24</td>\n",
       "      <td>1085</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Li6.7La3Zr2Ga0.1O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1021/acsami.6b13902', 'doi':...</td>\n",
       "      <td>Mixed Cubic / Tetragonal</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>25</td>\n",
       "      <td>1100</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Li6La2Ta1Nb1Ba1O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1007/s11581-013-0863-8', 'do...</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>25</td>\n",
       "      <td>900</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Li5.2La2Nb1.9Y0.1O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1021/jp304737x', 'doi': '10....</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>25</td>\n",
       "      <td>900</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>Ia3d</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Li7.06La3Zr1.94Y0.06O12</td>\n",
       "      <td>[{'name': 'SINTERING', 'details': [{'name': 'S...</td>\n",
       "      <td>[{'citation': '10.1149/2.088308jes', 'doi': '1...</td>\n",
       "      <td>Cubic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10^-4 S/cm</td>\n",
       "      <td>25</td>\n",
       "      <td>950</td>\n",
       "      <td>$\\circ$C</td>\n",
       "      <td>Ia3d</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               chemicalFormula  \\\n",
       "1          Li6.1La3Zr2Ga0.3O12   \n",
       "2            Li7La2Ta2Ba1O12.5   \n",
       "3              Li6La2Nb2Ba1O12   \n",
       "4   Li6.4La3Zr2Ga0.15Al0.05O12   \n",
       "5                 Li7La3Zr2O12   \n",
       "6          Li6.4La3Zr2Ga0.2O12   \n",
       "7          Li6.7La3Zr2Ga0.1O12   \n",
       "8           Li6La2Ta1Nb1Ba1O12   \n",
       "9         Li5.2La2Nb1.9Y0.1O12   \n",
       "10     Li7.06La3Zr1.94Y0.06O12   \n",
       "\n",
       "                                          preparation  \\\n",
       "1   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "2   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "3   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "4   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "5   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "6   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "7   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "8   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "9   [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "10  [{'name': 'SINTERING', 'details': [{'name': 'S...   \n",
       "\n",
       "                                           references  \\\n",
       "1   [{'citation': '10.1021/acsami.6b13902', 'doi':...   \n",
       "2   [{'citation': '10.1007/s00339-008-4494-2', 'do...   \n",
       "3   [{'citation': '10.1149/1.2800764', 'doi': '10....   \n",
       "4   [{'citation': '10.1021/acs.chemmater.6b00579',...   \n",
       "5   [{'citation': '10.1021/acs.inorgchem.5b00184',...   \n",
       "6   [{'citation': '10.1021/cm5008069', 'doi': '10....   \n",
       "7   [{'citation': '10.1021/acsami.6b13902', 'doi':...   \n",
       "8   [{'citation': '10.1007/s11581-013-0863-8', 'do...   \n",
       "9   [{'citation': '10.1021/jp304737x', 'doi': '10....   \n",
       "10  [{'citation': '10.1149/2.088308jes', 'doi': '1...   \n",
       "\n",
       "   Crystallographic Structure Ionic Conductivity Ionic Conductivity-units  \\\n",
       "1                       Cubic               11.2               10^-4 S/cm   \n",
       "2                       Cubic               1.01               10^-4 S/cm   \n",
       "3                       Cubic              0.155               10^-4 S/cm   \n",
       "4                       Cubic                8.8               10^-4 S/cm   \n",
       "5                       Cubic               1.74               10^-4 S/cm   \n",
       "6                       Cubic                  9               10^-4 S/cm   \n",
       "7    Mixed Cubic / Tetragonal               0.25               10^-4 S/cm   \n",
       "8                       Cubic             0.0309               10^-4 S/cm   \n",
       "9                       Cubic                0.2               10^-4 S/cm   \n",
       "10                      Cubic               0.01               10^-4 S/cm   \n",
       "\n",
       "   Measuring Temperature Sintering Temperature Sintering Temperature-units  \\\n",
       "1                     25                  1100                    $\\circ$C   \n",
       "2                     50                   900                    $\\circ$C   \n",
       "3                     17                   950                    $\\circ$C   \n",
       "4                     20                  1230                    $\\circ$C   \n",
       "5                     25                   900                    $\\circ$C   \n",
       "6                     24                  1085                    $\\circ$C   \n",
       "7                     25                  1100                    $\\circ$C   \n",
       "8                     25                   900                    $\\circ$C   \n",
       "9                     25                   900                    $\\circ$C   \n",
       "10                    25                   950                    $\\circ$C   \n",
       "\n",
       "   Space Group Year Published  \n",
       "1         Ia3d           2017  \n",
       "2         Ia3d           2008  \n",
       "3          NaN           2008  \n",
       "4         I43d           2016  \n",
       "5         Ia3d           2015  \n",
       "6          NaN           2014  \n",
       "7          NaN           2017  \n",
       "8          NaN           2013  \n",
       "9         Ia3d           2012  \n",
       "10        Ia3d           2013  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cdr = CitrineDataRetrieval(apikey) # Citrine Key\n",
    "\n",
    "data = cdr.get_dataframe(criteria={'data_set_id': 184812}, print_properties_options=False) # LLZO Database\n",
    "property_interest = 'Ionic Conductivity' # Property to be queried\n",
    "\n",
    "display(data.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a utility function that will transform the <i>chemicalFormula</i> column into a Matminer composition object, which will be then used to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_composition(c): # Function to get compositions from chemical formula using pymatgen\n",
    "    try:\n",
    "        return Composition(c)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the utility function to transform the <i>chemicalFormula</i> column, and we'll typecast relevant columns into numeric types.\n",
    "<br>\n",
    "For this specific application, we'll introduce some filters for the dataframe. We are interested in measurements in structures that are cubic and measured at room temperature (Defined as 18°C < T < 30°C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['composition'] = data['chemicalFormula'].apply(get_composition) # Transformation of chemicalformula string into Matminer composition\n",
    "data['Measuring Temperature'] = pd.to_numeric(data['Measuring Temperature'], errors='coerce') # Transformation of Measuring Temp dataframe column from type <str> to a numberic type <int>\n",
    "data[property_interest] = pd.to_numeric(data[property_interest], errors='coerce') # Transformation of our property of interest dataframe column from type <str> to a numberic type <int>\n",
    "data[\"Year Published\"] = pd.to_numeric(data[\"Year Published\"], errors='coerce') # Transformation of our property of interest dataframe column from type <str> to a numberic type <int>\n",
    "\n",
    "data = data[data['Crystallographic Structure'] == 'Cubic'] # Filter all non-cubic structures\n",
    "data = data[data['Measuring Temperature']<30] # Filter all high temperature measurements (over room temperature)\n",
    "data = data[data['Measuring Temperature']>18] # Filter all low temperature measurements (over room temperature)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True) # Reindexing of dataframe rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before removing duplicates, we will store all the experimental values for compositions that include Tantalum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_indexes = []\n",
    "for _ in range(len(data)):\n",
    "    if (\"Zr\" in data['composition'][_] and \"Ta\" in data['composition'][_] and len(data['composition'][_])==5):\n",
    "        ta_indexes.append(_)\n",
    "    elif (\"Zr\" in data['composition'][_] and len(data['composition'][_])==4):\n",
    "        ta_indexes.append(_) \n",
    "        \n",
    "ta_dataframe = data[data.index.isin(ta_indexes)]\n",
    "#display(ta_dataframe)        \n",
    "        \n",
    "x_values_exp = [_[\"Ta\"] for _ in list(ta_dataframe['composition'])]\n",
    "y_values_exp = list(ta_dataframe['Ionic Conductivity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce noise in the neural network and deal with the inconsistencies in the data, we will filter repeated composition values from different measurements and replace the value for ionic conductivity with the median of the values. Similar approaches have been implemented in this [paper](https://iopscience.iop.org/article/10.1088/1361-651X/aaf8ca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_indexes = data[data.duplicated(subset = data.columns.tolist()[0], keep=False)].index.tolist()\n",
    "\n",
    "dup_dataframe =data[data.duplicated(subset = data.columns.tolist()[0], keep=False)]\n",
    "\n",
    "duplicates = [[dup_dataframe.iloc[x][0], dup_dataframe.iloc[x][4], dup_indexes[x], dup_dataframe.iloc[x][-2]] for x in range(len(dup_dataframe.index))]\n",
    "duplicate_compositions = {k: [] for k in set([dup_dataframe.iloc[x][0] for x in range(len(dup_dataframe.index))])}\n",
    "duplicate_indexes = {k: [] for k in set([dup_dataframe.iloc[x][0] for x in range(len(dup_dataframe.index))])}\n",
    "duplicate_years = {k: [] for k in set([dup_dataframe.iloc[x][0] for x in range(len(dup_dataframe.index))])}\n",
    "\n",
    "for _ in duplicates:\n",
    "    duplicate_compositions[_[0]].append(_[1])\n",
    "    duplicate_indexes[_[0]].append(_[2]) \n",
    "    duplicate_years[_[0]].append(_[3])     \n",
    "\n",
    "for k in duplicate_compositions:\n",
    "    \n",
    "    duplicate_compositions[k] = np.median(duplicate_compositions[k])\n",
    "    data.at[duplicate_indexes[k][0], 'Ionic Conductivity'] = duplicate_compositions[k]  \n",
    "\n",
    "    duplicate_years[k] = np.min(duplicate_years[k])\n",
    "    data.at[duplicate_indexes[k][0], 'Year Published'] = duplicate_years[k]      \n",
    "    data = data.drop(duplicate_indexes[k][1:], axis = 0)\n",
    "\n",
    "data = data.reset_index()\n",
    "data = data.drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates, we will query the dataset for the values that were substituted for the Tantalum compositions, the median of the experimental values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_indexes = []\n",
    "for _ in range(len(data)):\n",
    "    if (\"Zr\" in data['composition'][_] and \"Ta\" in data['composition'][_] and len(data['composition'][_])==5):\n",
    "        ta_indexes.append(_)\n",
    "    elif (\"Zr\" in data['composition'][_] and len(data['composition'][_])==4):\n",
    "        ta_indexes.append(_)        \n",
    "        \n",
    "ta_dataframe = data[data.index.isin(ta_indexes)]\n",
    "#display(ta_dataframe)\n",
    "\n",
    "x_values_dupmed = [_[\"Ta\"] for _ in list(ta_dataframe['composition'])]\n",
    "y_values_dupmed = list(ta_dataframe['Ionic Conductivity'])\n",
    "\n",
    "sort_dupmed = list(zip(x_values_dupmed, y_values_dupmed))\n",
    "sort_dupmed = sorted(sort_dupmed, key = lambda t: t[0])\n",
    "\n",
    "x_values_dupmed = [item[0] for item in sort_dupmed ] \n",
    "y_values_dupmed = [item[1] for item in sort_dupmed ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell produces a breakdown of the number of elements in the oxides compositions and a distribution of the elements present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Li': 100, 'La': 100, 'O': 100, 'Zr': 70, 'Nb': 40, 'Ta': 29, 'Ga': 16, 'Y': 15, 'Sr': 11, 'Ba': 10, 'Gd': 10, 'Ca': 7, 'Bi': 6, 'Al': 5, 'Sc': 3, 'Nd': 3, 'Sb': 2})\n",
      "Counter({5: 62, 6: 31, 4: 6, 7: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "freq = data[\"composition\"]\n",
    "list_freq = []\n",
    "\n",
    "for _ in freq:\n",
    "    a = [str(x) for x in _]\n",
    "    list_freq.append(a)\n",
    "    \n",
    "list_freq_flat = [item for sublist in list_freq for item in sublist]  \n",
    "listfreqctr = collections.Counter(list_freq_flat)\n",
    "print(listfreqctr)\n",
    "    \n",
    "lengths = list(map(len,list_freq))\n",
    "lenctr = collections.Counter(lengths)\n",
    "\n",
    "print(lenctr)\n",
    "# print(type(freq[0]))\n",
    "# print(freq[0])\n",
    "# print(list(freq[0])[0])\n",
    "\n",
    "# print(type(list(freq[0])[0]))\n",
    "# print(str(list(freq[0])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Matminer Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b13ba04248243739ccb34dd7ad557ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='MultipleFeaturizer', style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(100, 105)\n"
     ]
    }
   ],
   "source": [
    "f =  MultipleFeaturizer([cf.Stoichiometry(), cf.ElementProperty.from_preset(\"magpie\"), cf.ValenceOrbital(props=['avg']), cf.ElementFraction()]) # Featurizers\n",
    "\n",
    "X = np.array(f.featurize_many(data['composition'], ignore_errors=True)) # Array to store such features\n",
    "\n",
    "measuring_temp_array = np.array(data['Measuring Temperature']).reshape(-1,1) # Here we are stacking the Measuring temperature numpy array into the features previously calculated to add it as a descriptor. \n",
    "X = np.hstack((X,measuring_temp_array))\n",
    "\n",
    "y = data[property_interest].values # Separate the value we want to predict to use as labels.\n",
    "years = data[\"Year Published\"].values\n",
    "\n",
    "# This code is to drop columns with std = 0. \n",
    "x_df = pd.DataFrame(X)\n",
    "x_df = x_df.loc[:, x_df.std() != 0]\n",
    "print(x_df.shape) # This shape is (#Entries, #Descriptors per entry)\n",
    "\n",
    "# This code is to drop columns with std = 0. \n",
    "x_df_prior = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 3. Regression Models (Architecture 1)\n",
    "\n",
    "We will start by creating a models for regression with all these entries and descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks (90-60-30-1)\n",
    "\n",
    "\n",
    "We set the architecture of the sequential feed-forward neural network we'll test. Weights are initialized with a Random Normal distribution and biases are initialized at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this training function with a validation mean absolute error (mae) stopping function to train the model. A 10% validation set is set to be taken from the training.\n",
    "<br>\n",
    "A figure of training mae vs validation mae is shown. Overfitting occurs when the validation mae starts to increase, so we revert the weights to those of the best epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0\n",
      "WARNING:tensorflow:From /apps/share64/debian7/anaconda/anaconda-6/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /apps/share64/debian7/anaconda/anaconda-6/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Current Epoch: 3282 Training Loss: 0.191564                                       \n",
      " Test Loss:  1.2104456424713135\n",
      "Split 1\n",
      "Current Epoch: 1049 Training Loss: 0.273764                                       \n",
      " Test Loss:  1.5149542093276978\n",
      "Split 2\n",
      "Current Epoch: 1049 Training Loss: 0.166175                                       \n",
      " Test Loss:  1.3422971963882446\n",
      "Split 3\n",
      "Current Epoch: 1038 Training Loss: 0.144613                                       \n",
      " Test Loss:  4.111766815185547\n",
      "Split 4\n",
      "Current Epoch: 1145 Training Loss: 0.167278                                       \n",
      " Test Loss:  0.6627296805381775\n",
      "Split 5\n",
      "Current Epoch: 1055 Training Loss: 0.246246                                       \n",
      " Test Loss:  0.6004053354263306\n",
      "Split 6\n",
      "Current Epoch: 1364 Training Loss: 0.229622                                       \n",
      " Test Loss:  0.5010675191879272\n",
      "Split 7\n",
      "Current Epoch: 2941 Training Loss: 0.134187                                       \n",
      " Test Loss:  2.151979446411133\n",
      "Split 8\n",
      "Current Epoch: 1073 Training Loss: 0.243311                                       \n",
      " Test Loss:  1.2764936685562134\n",
      "Split 9\n",
      "Current Epoch: 1052 Training Loss: 0.158978                                       \n",
      " Test Loss:  1.047703504562378\n",
      "Mean AVG MAE - 10 Splits: 1.441984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "for _i in range(10):\n",
    "    print(\"Split\", _i)\n",
    "\n",
    "    # DATA SPLIT AND NORMALIZATION\n",
    "    all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "    train_values, test_values = np.split(all_values, [index_split_at])\n",
    "    train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "    feature_mean = np.mean(train_values, axis=0)\n",
    "    feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "    train_values = (train_values - feature_mean)/ (feature_std)\n",
    "    test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "    # NETWORK ARCHITECTURE\n",
    "\n",
    "    neuralnetwork_model = Sequential()\n",
    "    neuralnetwork_model.add(Dense(90, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "    neuralnetwork_model.add(Dense(30, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "    neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "\n",
    "    neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "    [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "\n",
    "    print(\"\\n Test Loss: \", loss)\n",
    "    mae_st.append(mae)\n",
    "\n",
    "print(\"Mean AVG MAE - 10 Splits: %f\"%(np.mean(mae_st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a form of regularization, we will be exploring three options: Dropout, and the L1/L2 Layer weight regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Testing 10% Dropout \n",
      "Split 0\n",
      "WARNING:tensorflow:From /apps/share64/debian7/anaconda/anaconda-6/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Current Epoch: 3150 Training Loss: 0.375278                                       \n",
      " Test Loss:  1.0965170860290527\n",
      "Split 1\n",
      "Current Epoch: 1363 Training Loss: 0.439426                                       \n",
      " Test Loss:  1.3251070976257324\n",
      "Split 2\n",
      "Current Epoch: 1071 Training Loss: 0.396858                                       \n",
      " Test Loss:  1.0273689031600952\n",
      "Split 3\n",
      "Current Epoch: 1065 Training Loss: 0.426560                                       \n",
      " Test Loss:  3.0503926277160645\n",
      "Split 4\n",
      "Current Epoch: 1205 Training Loss: 0.426909                                       \n",
      " Test Loss:  0.5919569730758667\n",
      "Split 5\n",
      "Current Epoch: 1042 Training Loss: 0.616624                                       \n",
      " Test Loss:  0.48775404691696167\n",
      "Split 6\n",
      "Current Epoch: 1365 Training Loss: 0.451308                                       \n",
      " Test Loss:  0.5137383341789246\n",
      "Split 7\n",
      "Current Epoch: 1423 Training Loss: 0.573408                                       \n",
      " Test Loss:  0.9690800905227661\n",
      "Split 8\n",
      "Current Epoch: 1099 Training Loss: 0.455955                                       \n",
      " Test Loss:  1.1766647100448608\n",
      "Split 9\n",
      "Current Epoch: 1083 Training Loss: 0.420602                                       \n",
      " Test Loss:  0.724094033241272\n",
      "Mean AVG MAE - 10 Splits - 10% Dropout: 1.096267\n",
      "-----------------------------------------------\n",
      "Testing 20% Dropout \n",
      "Split 0\n",
      "Current Epoch: 1129 Training Loss: 0.480317                                       \n",
      " Test Loss:  0.6173087358474731\n",
      "Split 1\n",
      "Current Epoch: 1172 Training Loss: 0.656739                                       \n",
      " Test Loss:  0.7311413288116455\n",
      "Split 2\n",
      "Current Epoch: 1061 Training Loss: 0.559379                                       \n",
      " Test Loss:  0.8129287958145142\n",
      "Split 3\n",
      "Current Epoch: 1026 Training Loss: 0.572039                                       \n",
      " Test Loss:  2.1648213863372803\n",
      "Split 4\n",
      "Current Epoch: 1042 Training Loss: 0.563176                                       \n",
      " Test Loss:  1.716043472290039\n",
      "Split 5\n",
      "Current Epoch: 1508 Training Loss: 0.589796                                       \n",
      " Test Loss:  0.8298177719116211\n",
      "Split 6\n",
      "Current Epoch: 1321 Training Loss: 0.682156                                       \n",
      " Test Loss:  0.8104864358901978\n",
      "Split 7\n",
      "Current Epoch: 1033 Training Loss: 0.785743                                       \n",
      " Test Loss:  1.4999001026153564\n",
      "Split 8\n",
      "Current Epoch: 1074 Training Loss: 0.539906                                       \n",
      " Test Loss:  1.1151434183120728\n",
      "Split 9\n",
      "Current Epoch: 1046 Training Loss: 0.727505                                       \n",
      " Test Loss:  0.6271747350692749\n",
      "Mean AVG MAE - 10 Splits - 20% Dropout: 1.094372\n",
      "-----------------------------------------------\n",
      "Testing 30% Dropout \n",
      "Split 0\n",
      "Current Epoch: 1012 Training Loss: 0.680357                                       \n",
      " Test Loss:  2.5922467708587646\n",
      "Split 1\n",
      "Current Epoch: 1047 Training Loss: 0.806501                                       \n",
      " Test Loss:  1.4355523586273193\n",
      "Split 2\n",
      "Current Epoch: 1128 Training Loss: 0.784133                                       \n",
      " Test Loss:  0.6602790355682373\n",
      "Split 3\n",
      "Current Epoch: 1814 Training Loss: 0.609912                                       \n",
      " Test Loss:  1.6520397663116455\n",
      "Split 4\n",
      "Current Epoch: 1360 Training Loss: 0.663541                                       \n",
      " Test Loss:  1.703940749168396\n",
      "Split 5\n",
      "Current Epoch: 1036 Training Loss: 0.701855                                       \n",
      " Test Loss:  1.379949927330017\n",
      "Split 6\n",
      "Current Epoch: 1209 Training Loss: 0.828392                                       \n",
      " Test Loss:  1.4947887659072876\n",
      "Split 7\n",
      "Current Epoch: 1025 Training Loss: 0.601629                                       \n",
      " Test Loss:  1.376451849937439\n",
      "Split 8\n",
      "Current Epoch: 1084 Training Loss: 0.770204                                       \n",
      " Test Loss:  1.0892552137374878\n",
      "Split 9\n",
      "Current Epoch: 1112 Training Loss: 0.547178                                       \n",
      " Test Loss:  1.2186366319656372\n",
      "Mean AVG MAE - 10 Splits - 30% Dropout: 1.216353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "\n",
    "for drop in [0.1, 0.2,0.3]: \n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Testing %d%% Dropout \"%(drop*100))\n",
    "    \n",
    "    for _i in range(10):\n",
    "        print(\"Split\", _i)\n",
    "\n",
    "        # DATA SPLIT AND NORMALIZATION\n",
    "        all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "        train_values, test_values = np.split(all_values, [index_split_at])\n",
    "        train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "        feature_mean = np.mean(train_values, axis=0)\n",
    "        feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "        train_values = (train_values - feature_mean)/ (feature_std)\n",
    "        test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "        # NETWORK ARCHITECTURE\n",
    "\n",
    "        neuralnetwork_model = Sequential()\n",
    "        neuralnetwork_model.add(Dense(90, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "        neuralnetwork_model.add(Dropout(drop))\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "        neuralnetwork_model.add(Dropout(drop))\n",
    "        neuralnetwork_model.add(Dense(30, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "        neuralnetwork_model.add(Dropout(drop))\n",
    "        neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "\n",
    "        neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "        history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "        [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "        \n",
    "        \n",
    "        print(\"\\n Test Loss: \", loss)\n",
    "        mae_st.append(mae)\n",
    "\n",
    "    print(\"Mean AVG MAE - 10 Splits - %d%% Dropout: %f\"%(drop*100, np.mean(mae_st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Testing L2 Regularization: Penalty 0.000100 \n",
      "Split 0\n",
      "Current Epoch: 1741 Training Loss: 0.285532                                       \n",
      " Test Loss:  0.9004488587379456\n",
      "Split 1\n",
      "Current Epoch: 1057 Training Loss: 0.317207                                       \n",
      " Test Loss:  1.3330650329589844\n",
      "Split 2\n",
      "Current Epoch: 1051 Training Loss: 0.178800                                       \n",
      " Test Loss:  1.134377121925354\n",
      "Split 3\n",
      "Current Epoch: 1030 Training Loss: 0.183775                                       \n",
      " Test Loss:  4.5537543296813965\n",
      "Split 4\n",
      "Current Epoch: 1144 Training Loss: 0.186767                                       \n",
      " Test Loss:  0.7719566226005554\n",
      "Split 5\n",
      "Current Epoch: 1040 Training Loss: 0.282293                                       \n",
      " Test Loss:  0.6279588937759399\n",
      "Split 6\n",
      "Current Epoch: 1072 Training Loss: 0.298602                                       \n",
      " Test Loss:  0.4758690893650055\n",
      "Split 7\n",
      "Current Epoch: 1763 Training Loss: 0.232129                                       \n",
      " Test Loss:  1.7473540306091309\n",
      "Split 8\n",
      "Current Epoch: 1078 Training Loss: 0.245365                                       \n",
      " Test Loss:  1.2140294313430786\n",
      "Split 9\n",
      "Current Epoch: 1062 Training Loss: 0.176035                                       \n",
      " Test Loss:  1.075029969215393\n",
      "Mean AVG MAE - 10 Splits - L2 Regularization Penalty 0.000100: 1.373332 \n",
      "-----------------------------------------------\n",
      "Testing L2 Regularization: Penalty 0.000010 \n",
      "Split 0\n",
      "Current Epoch: 1031 Training Loss: 0.259107                                       \n",
      " Test Loss:  0.6073669195175171\n",
      "Split 1\n",
      "Current Epoch: 1080 Training Loss: 0.239150                                       \n",
      " Test Loss:  0.6788774132728577\n",
      "Split 2\n",
      "Current Epoch: 1051 Training Loss: 0.226808                                       \n",
      " Test Loss:  0.872313380241394\n",
      "Split 3\n",
      "Current Epoch: 1014 Training Loss: 0.132875                                       \n",
      " Test Loss:  2.272444248199463\n",
      "Split 4\n",
      "Current Epoch: 1041 Training Loss: 0.225252                                       \n",
      " Test Loss:  1.5258914232254028\n",
      "Split 5\n",
      "Current Epoch: 1117 Training Loss: 0.269177                                       \n",
      " Test Loss:  0.9263448119163513\n",
      "Split 6\n",
      "Current Epoch: 1766 Training Loss: 0.187342                                       \n",
      " Test Loss:  0.8957427144050598\n",
      "Split 7\n",
      "Current Epoch: 1040 Training Loss: 0.314191                                       \n",
      " Test Loss:  1.4508283138275146\n",
      "Split 8\n",
      "Current Epoch: 1059 Training Loss: 0.232792                                       \n",
      " Test Loss:  1.2266007661819458\n",
      "Split 9\n",
      "Current Epoch: 1027 Training Loss: 0.128300                                       \n",
      " Test Loss:  1.2393726110458374\n",
      "Mean AVG MAE - 10 Splits - L2 Regularization Penalty 0.000010: 1.270941 \n",
      "-----------------------------------------------\n",
      "Testing L2 Regularization: Penalty 0.000001 \n",
      "Split 0\n",
      "Current Epoch: 1024 Training Loss: 0.138686                                       \n",
      " Test Loss:  2.328810214996338\n",
      "Split 1\n",
      "Current Epoch: 1039 Training Loss: 0.269887                                       \n",
      " Test Loss:  1.364227294921875\n",
      "Split 2\n",
      "Current Epoch: 1046 Training Loss: 0.283509                                       \n",
      " Test Loss:  0.6737226843833923\n",
      "Split 3\n",
      "Current Epoch: 1099 Training Loss: 0.253927                                       \n",
      " Test Loss:  1.463009238243103\n",
      "Split 4\n",
      "Current Epoch: 1041 Training Loss: 0.226983                                       \n",
      " Test Loss:  2.9182791709899902\n",
      "Split 5\n",
      "Current Epoch: 1031 Training Loss: 0.121235                                       \n",
      " Test Loss:  1.485480785369873\n",
      "Split 6\n",
      "Current Epoch: 1057 Training Loss: 0.237943                                       \n",
      " Test Loss:  1.0374318361282349\n",
      "Split 7\n",
      "Current Epoch: 1023 Training Loss: 0.243471                                       \n",
      " Test Loss:  1.4899559020996094\n",
      "Split 8\n",
      "Current Epoch: 1041 Training Loss: 0.146346                                       \n",
      " Test Loss:  1.3785024881362915\n",
      "Split 9\n",
      "Current Epoch: 1036 Training Loss: 0.265911                                       \n",
      " Test Loss:  1.5586328506469727\n",
      "Mean AVG MAE - 10 Splits - L2 Regularization Penalty 0.000001: 1.370529 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "\n",
    "for reg in [1e-4, 1e-5, 1e-6]: \n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Testing L2 Regularization: Penalty %f \"%(reg))\n",
    "    \n",
    "    for _i in range(10):\n",
    "        print(\"Split\", _i)\n",
    "\n",
    "        # DATA SPLIT AND NORMALIZATION\n",
    "        all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "        train_values, test_values = np.split(all_values, [index_split_at])\n",
    "        train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "        feature_mean = np.mean(train_values, axis=0)\n",
    "        feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "        train_values = (train_values - feature_mean)/ (feature_std)\n",
    "        test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "        # NETWORK ARCHITECTURE\n",
    "\n",
    "        neuralnetwork_model = Sequential()\n",
    "        neuralnetwork_model.add(Dense(90, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg)))\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg) ))\n",
    "        neuralnetwork_model.add(Dense(30, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg) ))\n",
    "        neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg) ))\n",
    "\n",
    "        neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "        history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "        [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "        \n",
    "        \n",
    "        print(\"\\n Test Loss: \", loss)\n",
    "        mae_st.append(mae)\n",
    "\n",
    "    print(\"Mean AVG MAE - 10 Splits - L2 Regularization Penalty %f: %f \"%(reg, np.mean(mae_st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Testing L1 Regularization: Penalty 0.000100 \n",
      "Split 0\n",
      "Current Epoch: 1418 Training Loss: 0.375368                                       \n",
      " Test Loss:  0.9323380589485168\n",
      "Split 1\n",
      "Current Epoch: 1055 Training Loss: 0.413693                                       \n",
      " Test Loss:  1.5985743999481201\n",
      "Split 2\n",
      "Current Epoch: 1045 Training Loss: 0.225271                                       \n",
      " Test Loss:  1.343571662902832\n",
      "Split 3\n",
      "Current Epoch: 1042 Training Loss: 0.242513                                       \n",
      " Test Loss:  4.450512886047363\n",
      "Split 4\n",
      "Current Epoch: 1342 Training Loss: 0.306701                                       \n",
      " Test Loss:  0.7638652920722961\n",
      "Split 5\n",
      "Current Epoch: 1046 Training Loss: 0.346952                                       \n",
      " Test Loss:  0.6441807746887207\n",
      "Split 6\n",
      "Current Epoch: 1194 Training Loss: 0.376619                                       \n",
      " Test Loss:  0.6353306174278259\n",
      "Split 7\n",
      "Current Epoch: 2679 Training Loss: 0.267589                                       \n",
      " Test Loss:  2.117363214492798\n",
      "Split 8\n",
      "Current Epoch: 1119 Training Loss: 0.254307                                       \n",
      " Test Loss:  1.1993908882141113\n",
      "Split 9\n",
      "Current Epoch: 1058 Training Loss: 0.270295                                       \n",
      " Test Loss:  1.2041778564453125\n",
      "Mean AVG MAE - 10 Splits - L1 Regularization Penalty 0.000100: 1.413226 \n",
      "-----------------------------------------------\n",
      "Testing L1 Regularization: Penalty 0.000010 \n",
      "Split 0\n",
      "Current Epoch: 1032 Training Loss: 0.266112                                       \n",
      " Test Loss:  0.35302162170410156\n",
      "Split 1\n",
      "Current Epoch: 1080 Training Loss: 0.272482                                       \n",
      " Test Loss:  0.7637354731559753\n",
      "Split 2\n",
      "Current Epoch: 1048 Training Loss: 0.224694                                       \n",
      " Test Loss:  0.8148708939552307\n",
      "Split 3\n",
      "Current Epoch: 1048 Training Loss: 0.169882                                       \n",
      " Test Loss:  1.628781795501709\n",
      "Split 4\n",
      "Current Epoch: 1072 Training Loss: 0.231152                                       \n",
      " Test Loss:  1.299180030822754\n",
      "Split 5\n",
      "Current Epoch: 1100 Training Loss: 0.322063                                       \n",
      " Test Loss:  1.018977165222168\n",
      "Split 6\n",
      "Current Epoch: 1191 Training Loss: 0.228739                                       \n",
      " Test Loss:  0.9306621551513672\n",
      "Split 7\n",
      "Current Epoch: 1036 Training Loss: 0.348833                                       \n",
      " Test Loss:  1.54066801071167\n",
      "Split 8\n",
      "Current Epoch: 1059 Training Loss: 0.243242                                       \n",
      " Test Loss:  1.2184969186782837\n",
      "Split 9\n",
      "Current Epoch: 1037 Training Loss: 0.173450                                       \n",
      " Test Loss:  0.5097392797470093\n",
      "Mean AVG MAE - 10 Splits - L1 Regularization Penalty 0.000010: 1.205696 \n",
      "-----------------------------------------------\n",
      "Testing L1 Regularization: Penalty 0.000001 \n",
      "Split 0\n",
      "Current Epoch: 1023 Training Loss: 0.154711                                       \n",
      " Test Loss:  2.3498172760009766\n",
      "Split 1\n",
      "Current Epoch: 1046 Training Loss: 0.255633                                       \n",
      " Test Loss:  1.309876561164856\n",
      "Split 2\n",
      "Current Epoch: 1046 Training Loss: 0.233074                                       \n",
      " Test Loss:  0.759149968624115\n",
      "Split 3\n",
      "Current Epoch: 1110 Training Loss: 0.257095                                       \n",
      " Test Loss:  1.521252989768982\n",
      "Split 4\n",
      "Current Epoch: 1054 Training Loss: 0.256663                                       \n",
      " Test Loss:  1.859460473060608\n",
      "Split 5\n",
      "Current Epoch: 1021 Training Loss: 0.086711                                       \n",
      " Test Loss:  1.5284007787704468\n",
      "Split 6\n",
      "Current Epoch: 1052 Training Loss: 0.231057                                       \n",
      " Test Loss:  0.973583996295929\n",
      "Split 7\n",
      "Current Epoch: 1022 Training Loss: 0.240128                                       \n",
      " Test Loss:  1.5925931930541992\n",
      "Split 8\n",
      "Current Epoch: 1035 Training Loss: 0.104115                                       \n",
      " Test Loss:  1.4534423351287842\n",
      "Split 9\n",
      "Current Epoch: 1038 Training Loss: 0.201177                                       \n",
      " Test Loss:  1.4617727994918823\n",
      "Mean AVG MAE - 10 Splits - L1 Regularization Penalty 0.000001: 1.297112 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "\n",
    "for reg in [1e-4, 1e-5, 1e-6]: \n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Testing L1 Regularization: Penalty %f \"%(reg))\n",
    "    \n",
    "    for _i in range(10):\n",
    "        print(\"Split\", _i)\n",
    "\n",
    "        # DATA SPLIT AND NORMALIZATION\n",
    "        all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "        train_values, test_values = np.split(all_values, [index_split_at])\n",
    "        train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "        feature_mean = np.mean(train_values, axis=0)\n",
    "        feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "        train_values = (train_values - feature_mean)/ (feature_std)\n",
    "        test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "        # NETWORK ARCHITECTURE\n",
    "\n",
    "        neuralnetwork_model = Sequential()\n",
    "        neuralnetwork_model.add(Dense(90, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg)))\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg) ))\n",
    "        neuralnetwork_model.add(Dense(30, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg) ))\n",
    "        neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg) ))\n",
    "\n",
    "        neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "        history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "        [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "        \n",
    "        \n",
    "        print(\"\\n Test Loss: \", loss)\n",
    "        mae_st.append(mae)\n",
    "\n",
    "    print(\"Mean AVG MAE - 10 Splits - L1 Regularization Penalty %f: %f \"%(reg, np.mean(mae_st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 4. Regression Models (Architecture 2)\n",
    "\n",
    "We will start by creating a models for regression with all these entries and descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks (60-120-60-1)\n",
    "\n",
    "\n",
    "We set the architecture of the sequential feed-forward neural network we'll test. Weights are initialized with a Random Normal distribution and biases are initialized at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this training function with a validation mean absolute error (mae) stopping function to train the model. A 10% validation set is set to be taken from the training.\n",
    "<br>\n",
    "A figure of training mae vs validation mae is shown. Overfitting occurs when the validation mae starts to increase, so we revert the weights to those of the best epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AVG MAE - 10 Steps:  1.3464750319719314                                      \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "\n",
    "for _i in range(10):\n",
    "\n",
    "\n",
    "    # DATA SPLIT AND NORMALIZATION\n",
    "    all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "    train_values, test_values = np.split(all_values, [index_split_at])\n",
    "    train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "    feature_mean = np.mean(train_values, axis=0)\n",
    "    feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "    train_values = (train_values - feature_mean)/ (feature_std)\n",
    "    test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "    # NETWORK ARCHITECTURE\n",
    "    \n",
    "    neuralnetwork_model = Sequential()\n",
    "    neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "    neuralnetwork_model.add(Dense(120, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "    neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "    neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "\n",
    "    neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "    [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "    mae_st.append(mae)\n",
    "    \n",
    "print(\"Mean AVG MAE - 10 Steps: \", np.mean(mae_st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Testing 10% Dropout \n",
      "Split 0\n",
      "Current Epoch: 1562 Training Loss: 0.446494                                       \n",
      " Test Loss:  0.746110200881958\n",
      "Split 1\n",
      "Current Epoch: 1078 Training Loss: 0.438903                                       \n",
      " Test Loss:  1.3682829141616821\n",
      "Split 2\n",
      "Current Epoch: 1047 Training Loss: 0.397131                                       \n",
      " Test Loss:  1.1333160400390625\n",
      "Split 3\n",
      "Current Epoch: 1065 Training Loss: 0.400298                                       \n",
      " Test Loss:  3.641481399536133\n",
      "Split 4\n",
      "Current Epoch: 1108 Training Loss: 0.406301                                       \n",
      " Test Loss:  0.7020031213760376\n",
      "Split 5\n",
      "Current Epoch: 1071 Training Loss: 0.405413                                       \n",
      " Test Loss:  0.6211980581283569\n",
      "Split 6\n",
      "Current Epoch: 1419 Training Loss: 0.413708                                       \n",
      " Test Loss:  0.5984992384910583\n",
      "Split 7\n",
      "Current Epoch: 1847 Training Loss: 0.475434                                       \n",
      " Test Loss:  1.0804760456085205\n",
      "Split 8\n",
      "Current Epoch: 1081 Training Loss: 0.412550                                       \n",
      " Test Loss:  1.271085500717163\n",
      "Split 9\n",
      "Current Epoch: 1141 Training Loss: 0.381890                                       \n",
      " Test Loss:  0.9582311511039734\n",
      "Mean AVG MAE - 10 Splits - 10% Dropout: 1.212068\n",
      "-----------------------------------------------\n",
      "Testing 20% Dropout \n",
      "Split 0\n",
      "Current Epoch: 1030 Training Loss: 0.584661                                       \n",
      " Test Loss:  0.3421616852283478\n",
      "Split 1\n",
      "Current Epoch: 1106 Training Loss: 0.499110                                       \n",
      " Test Loss:  0.7502383589744568\n",
      "Split 2\n",
      "Current Epoch: 1058 Training Loss: 0.401054                                       \n",
      " Test Loss:  0.5616129040718079\n",
      "Split 3\n",
      "Current Epoch: 1019 Training Loss: 0.471124                                       \n",
      " Test Loss:  2.1308531761169434\n",
      "Split 4\n",
      "Current Epoch: 1037 Training Loss: 0.551261                                       \n",
      " Test Loss:  1.5844929218292236\n",
      "Split 5\n",
      "Current Epoch: 1092 Training Loss: 0.493652                                       \n",
      " Test Loss:  0.8750079870223999\n",
      "Split 6\n",
      "Current Epoch: 1299 Training Loss: 0.473493                                       \n",
      " Test Loss:  0.6437209248542786\n",
      "Split 7\n",
      "Current Epoch: 1030 Training Loss: 0.650987                                       \n",
      " Test Loss:  1.2142789363861084\n",
      "Split 8\n",
      "Current Epoch: 1047 Training Loss: 0.532546                                       \n",
      " Test Loss:  1.3139857053756714\n",
      "Split 9\n",
      "Current Epoch: 1050 Training Loss: 0.397713                                       \n",
      " Test Loss:  0.4167693555355072\n",
      "Mean AVG MAE - 10 Splits - 20% Dropout: 1.097690\n",
      "-----------------------------------------------\n",
      "Testing 30% Dropout \n",
      "Split 0\n",
      "Current Epoch: 1017 Training Loss: 0.498008                                       \n",
      " Test Loss:  2.2865569591522217\n",
      "Split 1\n",
      "Current Epoch: 1090 Training Loss: 0.603519                                       \n",
      " Test Loss:  1.2176506519317627\n",
      "Split 2\n",
      "Current Epoch: 2795 Training Loss: 0.607306                                       \n",
      " Test Loss:  0.7979342341423035\n",
      "Split 3\n",
      "Current Epoch: 4151 Training Loss: 0.553399                                       \n",
      " Test Loss:  1.456215500831604\n",
      "Split 4\n",
      "Current Epoch: 1048 Training Loss: 0.664260                                       \n",
      " Test Loss:  2.350598096847534\n",
      "Split 5\n",
      "Current Epoch: 1021 Training Loss: 0.671891                                       \n",
      " Test Loss:  1.5999075174331665\n",
      "Split 6\n",
      "Current Epoch: 1099 Training Loss: 0.675617                                       \n",
      " Test Loss:  1.0748835802078247\n",
      "Split 7\n",
      "Current Epoch: 1020 Training Loss: 0.628296                                       \n",
      " Test Loss:  1.1254342794418335\n",
      "Split 8\n",
      "Current Epoch: 1064 Training Loss: 0.575303                                       \n",
      " Test Loss:  1.1931092739105225\n",
      "Split 9\n",
      "Current Epoch: 1039 Training Loss: 0.565892                                       \n",
      " Test Loss:  1.585512399673462\n",
      "Mean AVG MAE - 10 Splits - 30% Dropout: 1.221387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "\n",
    "for drop in [0.1, 0.2,0.3]: \n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Testing %d%% Dropout \"%(drop*100))\n",
    "    \n",
    "    for _i in range(10):\n",
    "        print(\"Split\", _i)\n",
    "\n",
    "        # DATA SPLIT AND NORMALIZATION\n",
    "        all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "        train_values, test_values = np.split(all_values, [index_split_at])\n",
    "        train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "        feature_mean = np.mean(train_values, axis=0)\n",
    "        feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "        train_values = (train_values - feature_mean)/ (feature_std)\n",
    "        test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "        # NETWORK ARCHITECTURE\n",
    "\n",
    "        neuralnetwork_model = Sequential()\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "        neuralnetwork_model.add(Dropout(drop))\n",
    "        neuralnetwork_model.add(Dense(120, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "        neuralnetwork_model.add(Dropout(drop))\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "        neuralnetwork_model.add(Dropout(drop))\n",
    "        neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init ))\n",
    "\n",
    "        neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "        history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "        [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "        \n",
    "        \n",
    "        print(\"\\n Test Loss: \", loss)\n",
    "        mae_st.append(mae)\n",
    "\n",
    "    print(\"Mean AVG MAE - 10 Splits - %d%% Dropout: %f\"%(drop*100, np.mean(mae_st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Testing L2 Regularization: Penalty 0.000100 \n",
      "Split 0\n",
      "Current Epoch: 1582 Training Loss: 0.247738                                       \n",
      " Test Loss:  0.8544228076934814\n",
      "Split 1\n",
      "Current Epoch: 1201 Training Loss: 0.235488                                       \n",
      " Test Loss:  0.9531177878379822\n",
      "Split 2\n",
      "Current Epoch: 1059 Training Loss: 0.200203                                       \n",
      " Test Loss:  1.15052330493927\n",
      "Split 3\n",
      "Current Epoch: 1021 Training Loss: 0.143207                                       \n",
      " Test Loss:  4.000640392303467\n",
      "Split 4\n",
      "Current Epoch: 1101 Training Loss: 0.162880                                       \n",
      " Test Loss:  0.5941249132156372\n",
      "Split 5\n",
      "Current Epoch: 1080 Training Loss: 0.274732                                       \n",
      " Test Loss:  0.6986626982688904\n",
      "Split 6\n",
      "Current Epoch: 1063 Training Loss: 0.227749                                       \n",
      " Test Loss:  0.5755887627601624\n",
      "Split 7\n",
      "Current Epoch: 1386 Training Loss: 0.244750                                       \n",
      " Test Loss:  1.166393756866455\n",
      "Split 8\n",
      "Current Epoch: 1059 Training Loss: 0.188973                                       \n",
      " Test Loss:  1.2745736837387085\n",
      "Split 9\n",
      "Current Epoch: 1041 Training Loss: 0.151671                                       \n",
      " Test Loss:  0.9121925234794617\n",
      "Mean AVG MAE - 10 Splits - L2 Regularization Penalty 0.000100: 1.208795 \n",
      "-----------------------------------------------\n",
      "Testing L2 Regularization: Penalty 0.000010 \n",
      "Split 0\n",
      "Current Epoch: 1028 Training Loss: 0.235097                                       \n",
      " Test Loss:  0.5505657196044922\n",
      "Split 1\n",
      "Current Epoch: 1048 Training Loss: 0.251312                                       \n",
      " Test Loss:  1.1265925168991089\n",
      "Split 2\n",
      "Current Epoch: 1034 Training Loss: 0.206425                                       \n",
      " Test Loss:  0.9208992123603821\n",
      "Split 3\n",
      "Current Epoch: 1037 Training Loss: 0.108844                                       \n",
      " Test Loss:  1.6706494092941284\n",
      "Split 4\n",
      "Current Epoch: 1030 Training Loss: 0.242843                                       \n",
      " Test Loss:  1.3990490436553955\n",
      "Split 5\n",
      "Current Epoch: 1080 Training Loss: 0.255107                                       \n",
      " Test Loss:  0.9594548940658569\n",
      "Split 6\n",
      "Current Epoch: 1153 Training Loss: 0.177340                                       \n",
      " Test Loss:  0.8745628595352173\n",
      "Split 7\n",
      "Current Epoch: 1025 Training Loss: 0.317721                                       \n",
      " Test Loss:  1.504975438117981\n",
      "Split 8\n",
      "Current Epoch: 1032 Training Loss: 0.183035                                       \n",
      " Test Loss:  1.208139419555664\n",
      "Split 9\n",
      "Current Epoch: 1034 Training Loss: 0.155729                                       \n",
      " Test Loss:  0.6257748603820801\n",
      "Mean AVG MAE - 10 Splits - L2 Regularization Penalty 0.000010: 1.145948 \n",
      "-----------------------------------------------\n",
      "Testing L2 Regularization: Penalty 0.000001 \n",
      "Split 0\n",
      "Current Epoch: 1016 Training Loss: 0.114709                                       \n",
      " Test Loss:  2.2418978214263916\n",
      "Split 1\n",
      "Current Epoch: 1045 Training Loss: 0.215288                                       \n",
      " Test Loss:  1.1350678205490112\n",
      "Split 2\n",
      "Current Epoch: 1058 Training Loss: 0.252349                                       \n",
      " Test Loss:  0.8452615141868591\n",
      "Split 3\n",
      "Current Epoch: 1121 Training Loss: 0.250057                                       \n",
      " Test Loss:  1.5588231086730957\n",
      "Split 4\n",
      "Current Epoch: 1029 Training Loss: 0.169623                                       \n",
      " Test Loss:  2.656934976577759\n",
      "Split 5\n",
      "Current Epoch: 1032 Training Loss: 0.089783                                       \n",
      " Test Loss:  1.678408145904541\n",
      "Split 6\n",
      "Current Epoch: 1042 Training Loss: 0.203753                                       \n",
      " Test Loss:  1.0838543176651\n",
      "Split 7\n",
      "Current Epoch: 1018 Training Loss: 0.183719                                       \n",
      " Test Loss:  1.22371244430542\n",
      "Split 8\n",
      "Current Epoch: 1034 Training Loss: 0.120078                                       \n",
      " Test Loss:  1.3977775573730469\n",
      "Split 9\n",
      "Current Epoch: 1052 Training Loss: 0.221969                                       \n",
      " Test Loss:  1.22163724899292\n",
      "Mean AVG MAE - 10 Splits - L2 Regularization Penalty 0.000001: 1.265377 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "\n",
    "for reg in [1e-4, 1e-5, 1e-6]: \n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Testing L2 Regularization: Penalty %f \"%(reg))\n",
    "    \n",
    "    for _i in range(10):\n",
    "        print(\"Split\", _i)\n",
    "\n",
    "        # DATA SPLIT AND NORMALIZATION\n",
    "        all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "        train_values, test_values = np.split(all_values, [index_split_at])\n",
    "        train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "        feature_mean = np.mean(train_values, axis=0)\n",
    "        feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "        train_values = (train_values - feature_mean)/ (feature_std)\n",
    "        test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "        # NETWORK ARCHITECTURE\n",
    "\n",
    "        neuralnetwork_model = Sequential()\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg)))\n",
    "        neuralnetwork_model.add(Dense(120, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg) ))\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg) ))\n",
    "        neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l2(reg) ))\n",
    "\n",
    "        neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "        history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "        [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "        \n",
    "        \n",
    "        print(\"\\n Test Loss: \", loss)\n",
    "        mae_st.append(mae)\n",
    "\n",
    "    print(\"Mean AVG MAE - 10 Splits - L2 Regularization Penalty %f: %f \"%(reg, np.mean(mae_st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Testing L1 Regularization: Penalty 0.000100 \n",
      "Split 0\n",
      "Current Epoch: 1927 Training Loss: 0.329331                                       \n",
      " Test Loss:  1.0051826238632202\n",
      "Split 1\n",
      "Current Epoch: 1082 Training Loss: 0.313305                                       \n",
      " Test Loss:  1.359147071838379\n",
      "Split 2\n",
      "Current Epoch: 1081 Training Loss: 0.227364                                       \n",
      " Test Loss:  1.2636651992797852\n",
      "Split 3\n",
      "Current Epoch: 1024 Training Loss: 0.249700                                       \n",
      " Test Loss:  4.04002571105957\n",
      "Split 4\n",
      "Current Epoch: 1112 Training Loss: 0.277568                                       \n",
      " Test Loss:  0.852234423160553\n",
      "Split 5\n",
      "Current Epoch: 1032 Training Loss: 0.378093                                       \n",
      " Test Loss:  0.653967022895813\n",
      "Split 6\n",
      "Current Epoch: 1229 Training Loss: 0.319922                                       \n",
      " Test Loss:  0.5501052737236023\n",
      "Split 7\n",
      "Current Epoch: 1104 Training Loss: 0.339861                                       \n",
      " Test Loss:  0.8793411254882812\n",
      "Split 8\n",
      "Current Epoch: 1087 Training Loss: 0.254272                                       \n",
      " Test Loss:  1.2282229661941528\n",
      "Split 9\n",
      "Current Epoch: 1044 Training Loss: 0.227563                                       \n",
      " Test Loss:  1.1556494235992432\n",
      "Mean AVG MAE - 10 Splits - L1 Regularization Penalty 0.000100: 1.221191 \n",
      "-----------------------------------------------\n",
      "Testing L1 Regularization: Penalty 0.000010 \n",
      "Split 0\n",
      "Current Epoch: 1026 Training Loss: 0.246890                                       \n",
      " Test Loss:  0.5796247720718384\n",
      "Split 1\n",
      "Current Epoch: 1058 Training Loss: 0.270156                                       \n",
      " Test Loss:  0.721796989440918\n",
      "Split 2\n",
      "Current Epoch: 1042 Training Loss: 0.151760                                       \n",
      " Test Loss:  0.9889889359474182\n",
      "Split 3\n",
      "Current Epoch: 1035 Training Loss: 0.125713                                       \n",
      " Test Loss:  1.8661706447601318\n",
      "Split 4\n",
      "Current Epoch: 1035 Training Loss: 0.243742                                       \n",
      " Test Loss:  1.4867368936538696\n",
      "Split 5\n",
      "Current Epoch: 1056 Training Loss: 0.242511                                       \n",
      " Test Loss:  0.9092530608177185\n",
      "Split 6\n",
      "Current Epoch: 1191 Training Loss: 0.172630                                       \n",
      " Test Loss:  0.908867359161377\n",
      "Split 7\n",
      "Current Epoch: 1027 Training Loss: 0.243404                                       \n",
      " Test Loss:  1.3538280725479126\n",
      "Split 8\n",
      "Current Epoch: 1057 Training Loss: 0.197899                                       \n",
      " Test Loss:  1.242830514907837\n",
      "Split 9\n",
      "Current Epoch: 1034 Training Loss: 0.125011                                       \n",
      " Test Loss:  0.5807870626449585\n",
      "Mean AVG MAE - 10 Splits - L1 Regularization Penalty 0.000010: 1.137507 \n",
      "-----------------------------------------------\n",
      "Testing L1 Regularization: Penalty 0.000001 \n",
      "Split 0\n",
      "Current Epoch: 1016 Training Loss: 0.151162                                       \n",
      " Test Loss:  2.2906882762908936\n",
      "Split 1\n",
      "Current Epoch: 1044 Training Loss: 0.198765                                       \n",
      " Test Loss:  1.251871109008789\n",
      "Split 2\n",
      "Current Epoch: 1028 Training Loss: 0.190561                                       \n",
      " Test Loss:  0.7872701287269592\n",
      "Split 3\n",
      "Current Epoch: 2875 Training Loss: 0.122103                                       \n",
      " Test Loss:  1.6959679126739502\n",
      "Split 4\n",
      "Current Epoch: 1030 Training Loss: 0.209937                                       \n",
      " Test Loss:  2.431413173675537\n",
      "Split 5\n",
      "Current Epoch: 1031 Training Loss: 0.137053                                       \n",
      " Test Loss:  1.6798288822174072\n",
      "Split 6\n",
      "Current Epoch: 1042 Training Loss: 0.261801                                       \n",
      " Test Loss:  1.1350229978561401\n",
      "Split 7\n",
      "Current Epoch: 1019 Training Loss: 0.205333                                       \n",
      " Test Loss:  1.2292637825012207\n",
      "Split 8\n",
      "Current Epoch: 1227 Training Loss: 0.117210                                       \n",
      " Test Loss:  1.2001601457595825\n",
      "Split 9\n",
      "Current Epoch: 1040 Training Loss: 0.225432                                       \n",
      " Test Loss:  1.0877234935760498\n",
      "Mean AVG MAE - 10 Splits - L1 Regularization Penalty 0.000001: 1.250950 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# EXTRACTION OF THE DATA FOR THE DESCRIPTORS\n",
    "\n",
    "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "all_labels = y.copy()\n",
    "\n",
    "# SPLIT INDICATION FOR TRAIN/TEST SETS\n",
    "train_percent = 0.90\n",
    "index_split_at = int (train_percent * len(all_labels))\n",
    "\n",
    "\n",
    "# EARLY STOPPING CRITERIA\n",
    "#mae_es= keras.callbacks.EarlyStopping(monitor='mean_squared_error', min_delta=1e-8, patience=200, verbose=1, mode='auto', restore_best_weights=True)\n",
    "valmae_es= keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-10, patience=1000, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# NETWORK INITIALIZERS\n",
    "\n",
    "kernel_init = initializers.RandomNormal(seed=30)\n",
    "bias_init = initializers.Zeros()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "mae_st = []\n",
    "\n",
    "\n",
    "for reg in [1e-4, 1e-5, 1e-6]: \n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Testing L1 Regularization: Penalty %f \"%(reg))\n",
    "    \n",
    "    for _i in range(10):\n",
    "        print(\"Split\", _i)\n",
    "\n",
    "        # DATA SPLIT AND NORMALIZATION\n",
    "        all_values, all_labels = shuffle(all_values, all_labels, random_state=0)\n",
    "\n",
    "        train_values, test_values = np.split(all_values, [index_split_at])\n",
    "        train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
    "\n",
    "        feature_mean = np.mean(train_values, axis=0)\n",
    "        feature_std = np.std(train_values, axis=0)\n",
    "\n",
    "        train_values = (train_values - feature_mean)/ (feature_std)\n",
    "        test_values = (test_values - feature_mean)/ (feature_std)\n",
    "\n",
    "        # NETWORK ARCHITECTURE\n",
    "\n",
    "        neuralnetwork_model = Sequential()\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg)))\n",
    "        neuralnetwork_model.add(Dense(120, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg) ))\n",
    "        neuralnetwork_model.add(Dense(60, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg) ))\n",
    "        neuralnetwork_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init, kernel_regularizer=regularizers.l1(reg) ))\n",
    "\n",
    "        neuralnetwork_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "        history = neuralnetwork_model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose = False, callbacks=[PrintEpNum(), valmae_es]) #  \n",
    "\n",
    "        [loss, mae] = neuralnetwork_model.evaluate(test_values, test_labels, verbose=0)\n",
    "        \n",
    "        \n",
    "        print(\"\\n Test Loss: \", loss)\n",
    "        mae_st.append(mae)\n",
    "\n",
    "    print(\"Mean AVG MAE - 10 Splits - L1 Regularization Penalty %f: %f \"%(reg, np.mean(mae_st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
